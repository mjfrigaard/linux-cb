[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FM-Linux",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis project is under development. Thank you for your patience.\n\n\n\n\n\nWelcome!\nLinux is a powerful, versatile, and widely used operating system that runs on millions of devices worldwide, from smartphones to supercomputers.\n\nThis field manual covers an introduction to terminals and shells, the Linux filesystem hierarchy, moving around directories, file permissions, executing commands, and writing scripts with a text editor.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Background\nI began writing this book when I started a new position as a Posit System Administrator with CDC’s Enterprise Data, Analytics, and Visualization (EDAV) platform.1 EDAV houses multiple enterprise data science products (Databricks, Power BI, Tableau, SAS Viya, etc.), including Posit Workbench, Connect, and Package Manager.\nI’d been an R programmer for over ten years and spent the last five years working primarily on Shiny apps. My new responsibilities included (but were not limited to) maintaining, updating, monitoring and improving the Posit services for CDC users.\nI wrote the book I wish I had when transitioning from an R Developer to a System Administrator.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#background",
    "href": "preface.html#background",
    "title": "Preface",
    "section": "",
    "text": "“If there’s a book that you want to read, but it hasn’t been written yet, then you must write it.” - Toni Morrison",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#outline",
    "href": "preface.html#outline",
    "title": "Preface",
    "section": "Outline",
    "text": "Outline\nThe first half of this book gradually introduces the Linux environment, starting with some history and operating system basics before moving into more complex concepts. These sections (Set-Ups, Basics, Syntax, Text, and Shell Scripts) cover concepts and topics for users new to Linux.\nThe second half of the book covers case studies (or common tasks). If you’re familiar with Linux/Unix, command-line tools, and regular expressions, skip to these sections.\n\n\n\n\n\n\nWhat is in a Field Manual?\n\n\n\n\n\n\nA field manual (FM) is an educational tool that provides information, instructions, and techniques for various concepts and practices.\nFiels Manuals function as both reference material and living documentation, encouraging updates and adaptations based on the given situation, available technology, or broader environment.2\n\n\n\nExample improvised boobytrap made using a charge and blasting cap.\n\n\n\n\n\n\n\nPart One: Introduction\nThe Part 1: Introduction acquaints you with some background that formed the landscape of Unix systems. It explains the shell, a command-line interface (CLI), which is the gateway to leveraging the full potential of Linux.\n\nSet-Ups\nSetting up your Linux environment is crucial before diving into writing commands and scripts. Set-ups guide you through various options for setting up Virtual Machines, Shells, and Terminals.3\n\n\nThe Basics\nNavigating and managing files and folders are daily tasks for Linux users. Directories cover essential commands such as cd for changing directories, pwd to print the current directory, ls for listing files, and mkdir for making directories.\nThe Files chapter covers file manipulation commands like cp (copy), mv (move), rm (remove), and less for viewing file content, ensuring you can organize and manage your file system effectively. These commands are the building blocks for more complex operations in Linux.\nSystem goes over common system commands like top for monitoring processes in real-time, ps for listing currently running processes, kill to stop a process, df (Disk Free) to display disk space usage on all mounted filesystems, and du (Disk Usage) to estimate file space usage. These commands help efficiently manage the system’s processes and resources, ensuring a clear view of resource allocation and consumption.\n\n\nSyntax\nUnderstanding the shell syntax is vital for effectively communicating with a Linux system. The Syntax section demystifies the structure of commands, including how to differentiate between Commands, Arguments, and Options and manage inputs and outputs. This knowledge is critical to executing tasks reliably at the command line.\nPipes are a cornerstone of Linux productivity, enabling the output of one command to serve as the input to another. Syntax also covers how to combine commands using stdout and stdin, allowing for robust command chains that can perform complex tasks. Understanding pipes unlocks a higher level of command-line efficiency.\n\n\nManipulating Text\nLinux systems are renowned for their powerful text manipulation capabilities. The Text section covers using Symbols & Patterns to build regular expressions (regex). Manipulating Text introduces commands such as cat for displaying file contents, grep for searching within files, sorting data, sorting data, uniq for filtering unique lines, and cutting, pasting, and joining for editing files. Mastering these commands will allow you to handle and process text data efficiently. Text Editors cover Nano, vim, and Emacs.\n\n\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\nThank you for your patience.\n\n\n\n\n\n\nShell Scripts\nShell scripting is a powerful tool for automating repetitive tasks in Linux. This section introduces writing shell scripts, covering the basics of script creation, execution, and debugging. We’ll discuss how to automate simple tasks, making Linux tasks productive and enjoyable.\n\n\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\nThank you for your patience.\n\n\n\n\nThe Format chapter delves into the differences between these formats, guiding you on writing compatible scripts that can run across different Linux systems. Permissions explains how Linux permissions work and teaches you how to set and modify permissions to protect your data and system from unauthorized access.\n\n\n\nPart Two: Use Cases\n\n\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\nThank you for your patience.\n\n\n\n\nThe data files used in this book are documented in the data appendix.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#recap",
    "href": "preface.html#recap",
    "title": "Preface",
    "section": "Recap",
    "text": "Recap\nBy the end of this book, you’ll have a solid understanding of the Linux operating system. You’ll be equipped to navigate, manage files, write scripts, and set permissions confidently. Whether you’re looking to enhance your career prospects, manage a new system, or simply satisfy your curiosity, this book will be your companion on a fascinating journey into the world of Linux.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface",
    "section": "",
    "text": "Read more about EDAV and the Data Modernization Initiative here.↩︎\nThis definition is based on the U.S. Army Field Manuals (FMs).↩︎\nMost of these commands will work on any Shell (Zsh, Bash, etc.) program.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_one.html",
    "href": "part_one.html",
    "title": "Part 1: Introduction",
    "section": "",
    "text": "The Tale of Unix\nImagine Unix and Linux as cars in the vast landscape of automobile manufacturers. Our story begins in the late 1960s at AT&T’s Bell Labs—a significant moment in the history of technology. Unix was born out of a desire for a more flexible and portable operating system. In a time when computers were as big as rooms, Unix represented the custom-built car designed by a prestigious automaker. It was tailored to specific customers who needed top-end performance and reliability and was often used by large corporations or organizations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Part 1: Introduction]{style=\"font-size: 1.05em; font-weight: bold;\"}</span>"
    ]
  },
  {
    "objectID": "part_one.html#the-tale-of-unix",
    "href": "part_one.html#the-tale-of-unix",
    "title": "Part 1: Introduction",
    "section": "",
    "text": "Unix Philosophy\nUnix was a breath of fresh air, designed with a simplicity and elegance that set it apart. Its portability, the ability to run on different types of hardware, was a game-changer. Over time, Unix evolved into a comprehensive operating system, incorporating essential commands and operations and setting a new standard for efficiency and innovation.\nThe Unix philosophy (i.e., the foundational tools and techniques) became a blueprint for how computers could efficiently and securely manage tasks like organizing files or running software. The general tenets of the Unix philosophy are:\n\nFocus: Design programs to perform a single task effectively.\nSynergy: Programs should be designed to work well with others, ensuring programs can handle input and output via standard streams, allowing them to be combined in powerful ways.\nText: Unix tools are designed to read from and write to text streams.\nExtensibility: Tools should be designed with the expectation that users will extend them beyond their intended purpose.\nSimplicity: Simple, clear, straightforward code is preferred over complex code (even if it means sacrificing functionality).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Part 1: Introduction]{style=\"font-size: 1.05em; font-weight: bold;\"}</span>"
    ]
  },
  {
    "objectID": "part_one.html#the-emergence-of-linux",
    "href": "part_one.html#the-emergence-of-linux",
    "title": "Part 1: Introduction",
    "section": "The Emergence of Linux",
    "text": "The Emergence of Linux\nFast-forward a few decades to 1991, when a Finnish student named Linus Torvalds decided to create his free operating system kernel inspired by Unix. The Linux operating system was modeled after the principles of Unix but not directly derived from Unix source code and made freely available to anyone who wanted to use or improve it.\n\nOpen-source\nLinux flourished with the help of developers around the world. It’s a testament to what collaboration and shared goals can achieve. Whereas the development of Unix was largely driven by a specific group of organizations (like AT&T and Sun Microsystems), Linux was like a kit car designed by a community of enthusiasts, allowing anyone to assemble it with different parts, engines, and even paint (i.e., a fully customizable operating system). This global effort led to rapid evolution and various distributions, each interpreting and applying the Unix philosophy differently.\n\n\nModern Uses\nToday, Unix and Linux are everywhere. Although not as common on desktops as Windows or macOS, Linux distributions are the invisible forces behind much of the Internet and technology we interact with today:\n\nServers and Supercomputers: Linux is used by the majority of web servers powering the Internet and the world’s most powerful supercomputers due to its stability, security, and efficiency.\nSmartphones: Android, the most popular mobile operating system, is powered by a Linux kernel, making Linux the silent workhorse behind billions of smartphones.\nEmbedded Systems: From smartwatches to smart home devices, Linux is often the go-to choice for running embedded systems thanks to its scalability and low cost.\n\nUnix and Linux have grown from niche systems used by academics and researchers to foundational elements that power much of the digital world. They exemplify the power of open collaboration and innovation, showing how a small project or idea can grow and change the world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Part 1: Introduction]{style=\"font-size: 1.05em; font-weight: bold;\"}</span>"
    ]
  },
  {
    "objectID": "part_one.html#recap",
    "href": "part_one.html#recap",
    "title": "Part 1: Introduction",
    "section": "Recap",
    "text": "Recap\nTo summarize, both Unix and Linux are analogous to vehicles designed for transportation, but with a few important differences:\n\nUnix is like a high-end, custom-built car designed for a specific market, offering luxury, reliability, and exclusivity.\nLinux is like a kit car that can be built, modified, and customized however the user likes, with support from a large and active community of other enthusiasts.\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Part 1: Introduction]{style=\"font-size: 1.05em; font-weight: bold;\"}</span>"
    ]
  },
  {
    "objectID": "setups.html",
    "href": "setups.html",
    "title": "Set-Ups",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is being revised. Thank you for your patience.\n\n\n\n\nThis section details the practical aspects of preparing your environment to work with a Linux system. We’ll cover options for setting up a virtual machine and working with shells and terminals.\n\nVirtual Machines\nVirtual Machines are software-based emulations of a physical computer that allow users to run Linux distributions in an isolated environment, utilizing virtualized hardware resources from the host system.\n\n\n\n\n\n\n\n%%{init: {'theme': 'neutral', 'themeVariables': { 'fontFamily': 'monospace', \"fontSize\":\"16px\"}}}%%\n\nflowchart TD\n  VM(\"Virtual Machine\")\n  Linux(\"Linux Distribution 🐧\")\n  HostSystem{\"Host System\"}\n  CPU(\"Virtualized CPU\")\n  RAM(\"Virtualized RAM 🧠\")\n  Disk(\"Virtualized Disk 💽\")\n\n  HostSystem--&gt;VM\n  VM--&gt;Linux\n  VM--&gt;CPU\n  VM--&gt;RAM\n  VM--&gt;Disk\n\n\n\n Running Virtual Machines \n\n\n\nThis chapter covers virtualization software options, various Linux distributions, and provides an example of setting up Ubuntu on VirtualBox.\n\n\nShells\nShells are the command-line interpreters that allow users to interact with the operating system by executing commands and running scripts.\n\n\nTerminals\nTerminals\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups"
    ]
  },
  {
    "objectID": "vms.html",
    "href": "vms.html",
    "title": "Virtual Machines",
    "section": "",
    "text": "Virtualization Software\nVirtual machines (VMs) offer a flexible way to run Linux environments on top of your existing operating system, regardless of whether it’s Windows, macOS, or another Linux distribution. This chapter explores the setup process for virtual machines, highlighting popular VM software like VirtualBox and VMware.\nSetting up Linux on virtual machines (VMs) is a valuable technique for running different operating systems on a single physical machine, which is helpful for development, testing, server deployment, and more. Several virtualization tools are available, but the most popular ones include:",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "vms.html#virtualbox-ubuntu",
    "href": "vms.html#virtualbox-ubuntu",
    "title": "Virtual Machines",
    "section": " VirtualBox &  Ubuntu",
    "text": "VirtualBox &  Ubuntu\nIn this section we’ll cover how to setup VirtualBox and Ubuntu. We’ll download VirtualBox, select a Linux distribution, configuring the hardware and storage, and launch the virtual machine.\n\nDownload\nDownload and install VirtualBox from their website. I’m using macOS, so it’s a VirtualBox-7.0.20-163906-OSX.dmg file:\n\n\n\n\n\n\nFigure 2.1: Double-click the VirtualBox.pkg file to install\n\n\n\nAfter installing VirtualBox, download a Linux distribution (I’m using Ubuntu). I saved this .iso file in the same folder as the VirtualBox file:\n/Applications/VirtualBox/\n├── VirtualBox-7.0.20-163906-OSX.dmg\n└── ubuntu-24.04-desktop-amd64.iso\n\n1 directory, 2 files\nClick New in the VirtualBox manager and name the Virtual Machine and Operating System:\n\n\n\n\n\n\nFigure 2.2: New Virtual Machine on VirtualBox\n\n\n\n\n\nCreate\nCreate the new ISO image be adding the path to the downloaded Ubuntu file (ubuntu-24.04-desktop-amd64.iso).\n\n\n\n\n\n\nFigure 2.3: Creating new Ubuntu ISO image\n\n\n\nClick the Skip Unattended Installation checkbox and Next.\n\n\nConfigure\nIn this window we’ll configure the Ubuntu image. I’ve chosen to allocate 1/4 of the memory and processors on my machine to the image, but you can experiment with both settings to find the best configuration that fits your needs.\n\n\n\n\n\n\nFigure 2.4: Configure base memory and processors on Ubuntu ISO image\n\n\n\nIn the next window we’ll set the storage for this image (I’ve left the default).\n\n\n\n\n\n\nFigure 2.5: Configure hard disk on Ubuntu ISO image\n\n\n\nThe final window is a summary of our new virtual machine.\n\n\n\n\n\n\nFigure 2.6: Ubuntu ISO image Summary\n\n\n\nClick Finish to create the image.\n\n\nLaunch\nBack in the VirtualBox Manager, we can run this new virtual machine by clicking Start.\n\n\n\n\n\n\nFigure 2.7: Start virtual machine\n\n\n\nAfter clicking Start, a new window will open and attempt to start the virtual machine. Use the arrow keys to select Try or Install Ubuntu:\n\n\n\n\n\n\nFigure 2.8: Try or Install Ubuntu\n\n\n\nFinish the installation steps for Ubuntu for your liking.\n\n\n\n\n\n\nFigure 2.9: Complete Ubuntu Installation\n\n\n\nCreating virtual machines is an essential skill for those experimenting with Linux systems in an isolated environment.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "vms.html#recap",
    "href": "vms.html#recap",
    "title": "Virtual Machines",
    "section": "Recap",
    "text": "Recap\nThis chapter covered virtual machines and Linux distributions. Specifically, we discussed:\n\nSetting up Linux on virtual machines, highlighting the process and popular virtualization software options.\n\nOpen-source option: VirtualBox\nCommercial options: VMware Workstation, VMware Fusion, and Parallels Desktop\n\nThe key features of three major Linux distributions:\n\nUbuntu: known for its user-friendliness\nFedora: recognized for its cutting-edge technology\nCentOS: valued for its enterprise-level stability\n\n\nEach of these distributions brings its strengths and is tailored to different segments of the Linux user base, from desktop users and hobbyists to developers and enterprise clients.\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "shells.html",
    "href": "shells.html",
    "title": "Shells",
    "section": "",
    "text": "The Command Interpreter\nFramed in the context of the car manufacturers analogy, Shells are the car’s dashboards and controls–the interface between the driver and the car’s engine. Shells translate commands into actions a computer can understand and execute. Below I’ll cover two popular shells: Bash and Zsh.1\nA Shell is a program that interprets commands (i.e., the ‘command interpreter’) and acts as an intermediary between the user and the kernel of the operating system. Shells can be either command-line based or graphical.\nThe available shells are stored in /etc/shells, and we can view them with the command below:\ncat /etc/shells\n## # List of acceptable shells for chpass(1).\n## # Ftpd will not allow users to connect who are not using\n## # one of these shells.\n## \n## /bin/bash\n## /bin/csh\n## /bin/dash\n## /bin/ksh\n## /bin/sh\n## /bin/tcsh\n## /bin/zsh",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#the-command-interpreter",
    "href": "shells.html#the-command-interpreter",
    "title": "Shells",
    "section": "",
    "text": "Picking a shell\nTo change the default shell, use the chsh command (change shell). For example, if you want to switch to bash, use the command below:\n\nchsh -s /bin/bash\n\nYou can use the echo and ps commands to verify the shell you’re currently using:\n\necho $SHELL\n# /bin/bash\n\n\nps -p $$\n#   PID TTY           TIME CMD\n# 16082 ttys003    0:00.02 -bash",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#sec-bash",
    "href": "shells.html#sec-bash",
    "title": "Shells",
    "section": " Bash",
    "text": "Bash\nIntroduced in 1989, Bash, or the Bourne Again SHell has become the default command-line interface or shell for most Linux distributions. Incorporating features from the Korn shell (ksh) and the C shell (csh), Bash supports features like command history, tab completion, aliases and scripting tasks.2\n\nKey features\nProgramming\nBash includes an array of programming constructs for scripting:\n\nConditional statements (if, then, else, elif, fi)\n\nLooping statements (for, while, until)\n\nInteractive Command Line Editing\nBash provides an interactive command line editing environment where users can navigate and edit commands directly on the command line using Emacs or Vi editing modes.\nHistory expansion\nCommands can be re-executed by recalling them from the history.\nTab Completion\nBash supports tab completion for command names, file names, and even command arguments, speeding up the input process and reducing typos.\nComprehensive Job Control\n\nBackgrounding (&), foregrounding (fg), and job management (jobs, bg)\n\nStopping (suspending) processes and continuing them with kill and kill -CONT\n\nAliases\nUsers can create shorter commands to represent longer sequences of commands using aliases.\nShell Functions\nBash also supports more powerful functions that can take arguments like small scripts.\nScript Debugging\nBash scripts can be debugged using options like set -x to print commands and their arguments as they are executed, which is invaluable for troubleshooting scripts.\nEnvironment Control\n\nEnvironment variables configuration and management\nVariables are exported to make them available to sub-processes\n\nExpansion Capabilities\nBash supports several types of expansions that enhance its scripting capabilities:\n\nBrace expansion: {a,b,c}\nTilde expansion: ~ translates to the home directory.\nParameter and variable expansion: $name or ${name}\nArithmetic expansion: $(( expression )0\n\nHistory Features\nBash maintains a history of commands that users have executed, which can be navigated, searched, and reused. It also supports configuring the history size and behavior through various environment variables like HISTSIZE and HISTFILESIZE.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#zsh",
    "href": "shells.html#zsh",
    "title": "Shells",
    "section": " Zsh",
    "text": "Zsh\nZsh (Z Shell or ‘Oh My ZSH!’) is noted for its interactive features and is often used with customization frameworks. Zsh is a powerful command-line interpreter for Linux systems that serves as both a scriptable shell and an interactive command interpreter.3\n\nKey features\nCommand Line Editing\nZsh provides an advanced and customizable command-line editing environment. Users can configure key bindings and have extensive control over the text editing capabilities directly within the command prompt.\nTab Completion\nZsh has one of the most powerful tab completion systems. It supports:\n\nCompletion for command options and arguments.\nAutomatic listing of options when a tab is hit twice.\nContext-sensitive completion that can recognize patterns in filenames, history, running processes, hostnames, and more.\n\nThemes and Prompts\nZsh allows extensive customization of its prompt, supporting themes that can completely change the look of your command line. The prompt can include colors, content from shell variables, functions, and command outputs.\nScripting\nZsh scripting is robust, with features like arrays, associative arrays, and floating-point arithmetic which are not typically available in all shells. It enhances scripting capabilities and improves on the scripting syntax of the Bourne Shell.\nLoadable Modules\nZsh supports dynamically loadable modules, expanding its capabilities with features like:\n\nFTP client\nTCP and UDP socket operations\nAdvanced math functions\nFull-fledged regular expression matching\n\nImproved Variable Handling\nVariable handling in Zsh includes several enhancements like:\n\nBetter array handling\nAssociative arrays (similar to dictionaries in higher-level programming languages)\nEasier string manipulation and pattern matching\n\nSpell Check and Correction\nZsh can be configured to correct commands automatically if misspelled and to suggest corrections or alternatives. This feature helps in reducing syntax errors and improves user efficiency.\nExtended Globbing\nZsh’s file globbing allows for more complex pattern matching than traditional Linux shells. You can specify patterns in a more expressive and powerful way, which is particularly useful in scripts.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#customizations",
    "href": "shells.html#customizations",
    "title": "Shells",
    "section": "Customizations",
    "text": "Customizations\nCustomizing the shell environment can significantly enhance your productivity by tailoring it to your specific needs and preferences. The next sections cover come basic customizations for Bash and Zsh.\n\nPrompts\n Bash\nBash offers a wide range of customization options through the .bash_profile file, which is sourced every time a new terminal session is started.\nThe PS1 variable defines the appearance of your command prompt. Use export PS1= to customize the prompt in the ~/.bash_profile file:4\n\n\n\n\n\n\n# in ~/.bash_profile\nexport PS1=\"Bash@MacBook \\W\\$ \"\n\n\n# prompt\nBash@MacBook ~$ \n\n\n\nBelow we’ll customizes the prompt to show Bash in green, @MacBook in blue, and the current working directory (\\W) in red, followed by a dollar sign ($).\nDefining ANSI Colors in ~/.bash_profile\n\n# define colors\nGREEN=\"\\[\\e[32m\\]\"\nBLUE=\"\\[\\e[34m\\]\"\nRED=\"\\[\\e[31m\\]\"\nRESET=\"\\[\\e[0m\\]\"\n\n\nGREEN=\"\\[\\e[32m\\]\": ANSI escape code for green\n\nBLUE=\"\\[\\e[34m\\]\": ANSI escape code for blue\n\nRED=\"\\[\\e[31m\\]\": ANSI escape code for red\n\nRESET=\"\\[\\e[0m\\]\": Resets color back to default terminal color\n\nUsing Colors in PS1\n\n# customize PS1 prompt with colors\nexport PS1=\"${GREEN}Bash${RESET}${BLUE}@MacBook${RESET} ${RED}\\W${RESET}\\$ \"\n\n\n${GREEN}: Inserts green color before Bash\n\n${RESET}: Resets the color after Bash and MacBook to ensure that only Bash and MacBook are colored\n\n${BLUE}: Inserts the blue color before @MacBook\n\n${RED}: Inserts the red color before the working directory (\\W)\n\n${RESET} at the end ensures that the prompt returns to the default color after displaying the prompt\n\nAfter updating the ~/.bash_profile file, run source ~/.bash_profile to apply the changes to the current session.\n\n\n\nCustomized prompt on bash shell\n\n\n Zsh\nCustomizations in Zsh are similar to Bash, but they done using the .zshrc file. Zsh has PROMPT variable to customize the prompt, but with more advanced capabilities:\n# Define color variables\nGREEN=\"%F{green}\"\nBLUE=\"%F{blue}\"\nRED=\"%F{red}\"\nRESET=\"%f\"\n\n# Customize the prompt using the variables\nexport PROMPT=\"${GREEN}Zsh${RESET}${BLUE}@MacBook${RESET} ${RED}&lt;%~&gt;${RESET} \"\n\n\nAliases\nAliases allow you to create shortcuts for commonly used commands.\n Bash\nBash aliases: These aliases simplify the ls -la command to ll and git status to gs.\nalias ll='ls -la --color=auto'\nalias gs='git status'\n Zsh\nZsh Aliases: Zsh supports aliases in a similar manner to Bash.\nalias ll='ls -lh'\n\n\nFunctions\n Bash\nBash Functions: Bash functions allow you to create reusable scripts within your shell environment.\nfunction mkcd() {\n    mkdir -p \"$1\" && cd \"$1\";\n}\nThe mkcd function creates a directory and then navigates into it.\n Zsh\nZsh Functions: Zsh supports functions in a similar manner to Bash.\nfunction extract() {\n    if [ -f $1 ]; then\n        case $1 in\n            *.tar.bz2)   tar xjf $1   ;;\n            *.tar.gz)    tar xzf $1   ;;\n            *.bz2)       bunzip2 $1   ;;\n            *.rar)       unrar e $1   ;;\n            *.gz)        gunzip $1    ;;\n            *.tar)       tar xf $1    ;;\n            *.tbz2)      tar xjf $1   ;;\n            *.tgz)       tar xzf $1   ;;\n            *.zip)       unzip $1     ;;\n            *.Z)         uncompress $1;;\n            *.7z)        7z x $1      ;;\n            *)           echo \"don't know how to extract '$1'...\" ;;\n        esac\n    else\n        echo \"'$1' is not a valid file!\"\n    fi\n}\n\n\nAdvanced\nBelow we cover some advanced customizations.\n Bash\nBash Completion\nEnhance command-line efficiency by enabling tab completion for various commands.\nif [ -f /etc/bash_completion ]; then\n    . /etc/bash_completion\nfi\n Zsh\nOh My Zsh\nA popular framework for managing Zsh configuration, making it easy to apply themes and plugins.\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nOnce installed, you can easily switch themes by editing the ZSH_THEME variable in your .zshrc file.\nZSH_THEME=\"agnoster\"\nZsh Completions and Plugins\nZsh offers extensive tab-completion support and a variety of plugins that can be enabled for additional functionality.\nplugins=(git docker kubectl)\n\n\n\n\n\n\nChanging prompts in Zsh\n\n\n\n\n\n\n\nOpen your .zshrc file in your home directory with a text editor.\n~/.zshrc\nAdd or change the line that defines PROMPT. For example:\nexport PROMPT=\"Zsh@MacBook %1~%% \"\n\nThis sets a prompt similar to Bash’s example above.\nZsh@MacBook ~% \n\nSave the file and apply the changes by sourcing the profile:\nsource ~/.zshrc",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#recap",
    "href": "shells.html#recap",
    "title": "Shells",
    "section": "Recap",
    "text": "Recap\nIf Linux is the environment where the heavy lifting of computing happens–managing files, running programs, and controlling hardware–then the shell commands are the language and syntax that spell out what computational work happens.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "shells.html#footnotes",
    "href": "shells.html#footnotes",
    "title": "Shells",
    "section": "",
    "text": "The appendix also covers the key features and customizations of the Fish shell↩︎\nBash was the default command-line interface for Apple’s macOS (which is Linux-based) until the transition to zsh as the default shell in macOS Catalina.↩︎\nZsh is an extended version of Bash (Bourne Again SHell), with many improvements, and is fully compatible with the Bourne Shell.↩︎\n\\W prints the basename of the current working directory. You can also specify the username (\\u) and the host name up to the first period (\\h).↩︎",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shells</span>"
    ]
  },
  {
    "objectID": "terminals.html",
    "href": "terminals.html",
    "title": "Terminals",
    "section": "",
    "text": "GNOME\nIn Linux environments, the terms ‘Shell’ and ‘Terminal’ are commonly used and although they are related, they actually refer to different things: A terminal (or terminal emulator) is a software program that provides a text-based interface to the shell, the program that processes user commands–which might also involve calling other programs–and returns the output.\nBelow is an expanded look at some commonly used terminal emulators and their key features.\nGNOME is the default terminal emulator for the GNOME desktop environment, widely used in many Linux distributions.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Terminals</span>"
    ]
  },
  {
    "objectID": "terminals.html#gnome",
    "href": "terminals.html#gnome",
    "title": "Terminals",
    "section": "",
    "text": "Figure 4.1: Read more about the GNOME Terminal in the official documentation.\n\n\n\n\nKey features\nProfiles\nUsers can create multiple profiles, each with its own set of preferences, including colors, fonts, and keyboard shortcuts.\nTabs and Splitting\nSupports opening multiple tabs and can split the terminal window into multiple panes.\nTransparency and Backgrounds\nAllows setting background images and adjusting the transparency of the terminal window.\nCompatibility\nSupports UTF-8 for a wide range of characters, making it suitable for international use.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Terminals</span>"
    ]
  },
  {
    "objectID": "terminals.html#konsole",
    "href": "terminals.html#konsole",
    "title": "Terminals",
    "section": " Konsole",
    "text": "Konsole\nKonsole is part of the KDE desktop environment. It is known for its deep integration with KDE and its high degree of customizability.\n\n\n\n\n\n\nFigure 4.2: Read how to customize your Konsole terminal in KDE Users: Up Your Konsole Game by Al Williams\n\n\n\n\nKey features\nTabbed Interface\nAllows multiple tabs within a single window, facilitating multitasking.\nProfiles\nSupports multiple profiles, enabling different settings for each session.\nSplit Views\nUsers can split Konsole windows horizontally or vertically.\nTransparency and Theming\nSupports background transparency and themes, which can be customized easily.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Terminals</span>"
    ]
  },
  {
    "objectID": "terminals.html#iterm2",
    "href": "terminals.html#iterm2",
    "title": "Terminals",
    "section": " iTerm2",
    "text": "iTerm2\niTerm2 is a replacement for Terminal and the successor to iTerm for macOS. It offers features beyond what traditional terminals provide.\n\n\n\n\n\n\nFigure 4.3: Execution of feature and cowsay in iterm2 on macOS\n\n\n\n\nKey features\nSplit Panes\nUsers can divide iTerm2 into multiple panes, each with its own session.\nSearch\niTerm2 allows users to search through text and highlights occurrences.\nProfiles\nSupports detailed profiles, each with its custom colors, fonts, window transparency, and key bindings.\nAdvanced Paste Features\nOffers a paste history and allows pasting with escape codes to avoid issues with unintended command executions.\nMouseless Copy\niTerm2 lets you use keyboard shortcuts to select and copy text without needing the mouse.\nShell Integration\niTerm2 can integrate with the shell to display badges, track command statuses, and more.\nTrigger Support\nExecutes user-defined actions based on text output to the terminal.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Terminals</span>"
    ]
  },
  {
    "objectID": "terminals.html#recap",
    "href": "terminals.html#recap",
    "title": "Terminals",
    "section": "Recap",
    "text": "Recap\nEach of these terminal emulators offers unique features that cater to different needs and preferences, enhancing the user’s command-line experience. Whether you need deep customization, minimal resource usage, or advanced functionalities like search and shell integration, there’s a terminal emulator that fits the requirement.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Terminals</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Basics",
    "section": "",
    "text": "Directories\nThe next sections introduce some common topics and commands that will help you navigate folders (or directories), manipulate and manage files, and check system processes. These sections focus on executing single commands in a Terminal. In the next section we’ll dive into the Linux command syntax (i.e., arguments, options, and pipes).\nIn Linux, directories are special types of files that contain other files, including other directories. This ties into a fundamental concept in Linux: everything is a file. This means that not only are traditional files like documents and images considered files, but so are directories, devices, processes, and even network sockets.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#recap",
    "href": "basics.html#recap",
    "title": "Basics",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "dirs.html",
    "href": "dirs.html",
    "title": "Directories",
    "section": "",
    "text": "Root Directory\nWhen presented with a new map, the most important thing to find is your location on it (it’s hard to know where you’re going without knowing where you are). In Linux, directories are more than just containers for files; they are a critical part of the hierarchical file system, which is organized in a tree-like structure starting from the root directory (/).\nThe root directory is the topmost directory in the Linux file system hierarchy. All other directories and files are nested within it. It is represented by a single forward slash / and contains critical system directories like /bin, /etc, /home, and /var.\n%%{init: {'theme': 'neutral', 'themeVariables': { 'fontFamily': 'monospace', \"fontSize\":\"16px\"}}}%%\n\nflowchart TD\n  Root(&lt;code&gt;/&lt;/code&gt;)\n  bin(&lt;code&gt;/bin&lt;/code&gt;)\n  etc(&lt;code&gt;/etc&lt;/code&gt;)\n  home(&lt;code&gt;/home&lt;/code&gt;)\n  usr(&lt;code&gt;/usr&lt;/code&gt;)\n  var(&lt;code&gt;/var&lt;/code&gt;)\n  tmp(&lt;code&gt;/tmp&lt;/code&gt;)\n\n  Root --&gt; bin\n  Root --&gt; etc\n  Root --&gt; home\n  Root --&gt; usr\n  Root --&gt; var\n  Root --&gt; tmp\n\n\n Everything is a file",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#root-directory",
    "href": "dirs.html#root-directory",
    "title": "Directories",
    "section": "",
    "text": "Subdirectories\nBeneath the root directory, the Linux file system is organized into various subdirectories, each serving specific purposes:\n/bin – Contains essential user command binaries\n/etc – Stores system-wide configuration files.\n/home – Houses personal directories for each user.\n/var – Contains variable data like logs and spool files.\nIn Linux systems, the ~ represents the user’s ‘home’ directory.\n\n\nFile paths\nA file path is a character string specifying the unique location of a file or directory within the hierarchical file system. File paths can be absolute (starting from the root (/) directory) or relative (starting from the current (.) directory).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-nav-dirs",
    "href": "dirs.html#sec-nav-dirs",
    "title": "Directories",
    "section": "Navigate",
    "text": "Navigate\n\npwd\npwd (print working directory) tells you exactly where you are in the filesystem.\n\npwd \n# /Users/username/projects/books/fm-linux\n\n\nThe output from pwd is the file path to our local working directory.\n\ncd ~\npwd\n# /Users/username\n\n\n\ntree\nThe tree command will recursively print the directory structure of a file path in a ‘tree-like’ format, visually representing the hierarchy of files and directories.\nBelow is an example ‘folder tree’ for the current working directory returned from the pwd command above:\n\n# /Users/ \n#   └─ username/ -&gt; represented as '~'\n#        └─ project/ \n#             └─ books/ \n#                 └─ fm-linux/\n\nThe output from pwd is an absolute file path, and absolute file paths do not change regardless of the current working directory.\n\n\ncd\nMove from one directory to another with cd (change directory). For example, cd data takes you to the data folder inside our current working directory.\n\ncd data\n\nIn the command above, data is a relative file path. Relative file paths specify the location of a file relative to the current working directory.\n\n\n\n\n\n\nIf we view our current directory with tree after changing it to data, we see the current location listed as .:\n\n\ncd data \ntree -d\n# .\n# └── raw\n# \n# 2 directories\n\n\n\nIt’s important to notice the difference between absolute and relative paths, because it makes it easier to navigate the operating system and manipulate files and folders.\nFor example, can use a relative file path to view the report.txt file in the data folder with cat:\n\ncd data\ncat report.txt\n# my important information\n\ndata/report.txt is the relative file path (i.e., relative to the current working directory), and it’s meaning is based on the directory from which it is referenced.\n\n\nls\nls (list) lists the files and folders in a given location. In /bin, ls would show you the software tools available:\n\ncd /bin # change location\nls # what's in here?\n# [\n# bash\n# cat\n# chmod\n# cp\n# csh\n# dash\n# date\n# dd\n# df\n# echo\n# ed\n# expr\n# hostname\n# kill\n# ksh\n# launchctl\n# link\n# ln\n# ls\n# mkdir\n# mv\n# pax\n# ps\n# pwd\n# realpath\n# rm\n# rmdir\n# sh\n# sleep\n# stty\n# sync\n# tcsh\n# test\n# unlink\n# wait4path\n# zsh\n\n\n\nfind\nfind can be used to locate files or directories using the -type and -name options. The example below looks in the current working directory (.) for a folder named data:\n\nfind . -type d -name data\n# ./data",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-manage-dirs",
    "href": "dirs.html#sec-manage-dirs",
    "title": "Directories",
    "section": "Manage",
    "text": "Manage\nIn the Linux world, file and directory management is a fundamental skill. This chapter introduces some common commands that will allow you to create, copy, move, remove, and link files and directories.\n\nmkdir\nmkdir (Make Directory) builds a new folder wherever you tell it to, like making a new folder in data for inputs (data/in) or outputs (data/out)\n\nmkdir data/in\nmkdir data/out\n\nConfirm with tree -d (the -d is for directories):\n\ntree data -d\n# data\n# ├── in\n# └── out\n# \n# 3 directories\n\n\n\ncp\ncp duplicates files or folders. The cp command is used to copy files or directories from one location to another. Imagine having a file (binary_data.tsv) on your root (.) directory that you want to copy to the /data/in folder; you could use cp to make a duplicate.\n\ncp binary_data.tsv data/in/binary_data.tsv\n\nConfirm with tree\n\ntree data/in\n# data/in\n# └── binary_data.tsv\n# \n# 1 directory, 1 file\n\n\n\nmv\nmv, short for move, moves files or directories from one location to another. We’ll use it to move data/binary_data.tsv to data/out/binary_data.tsv:\n\n# move file\nmv data/in/binary_data.tsv data/out/binary_data.tsv \n\nConfirm move with tree:1\n\ntree data -P *.tsv\n# data\n# ├── in\n# └── out\n#     └── binary_data.tsv\n# \n# 3 directories, 1 file\n\nIt can also be used for renaming files.\n\n# rename file\nmv data/out/binary_data.tsv  data/out/bin_dat.tsv \n\n\n# confirm rename \ntree data/out\n# data/out\n# └── bin_dat.tsv\n# \n# 1 directory, 1 file\n\nmv is especially useful for organizing files and directories that are in the wrong place.\n\n\nrm\nThe rm command stands for remove and is used to delete files or directories.\n\n# remove doc folder\nrm data/out\n# rm: data/out: is a directory\n\nBy default, it won’t remove a directory without the -R or -r option.\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nIt’s important to note here that the command-line is not very forgiving. Using rm is a powerful action with significant consequences, as it permanently deletes files, akin to shredding documents. There’s usually no easy way to recover deleted files unless you have a backup.\n\n‘Linux is like a chainsaw. Chainsaws are powerful tools, and make many difficult tasks like cutting through thick logs quite easy. Unfortunately, this power comes with danger: chainsaws can cut just as easily through your leg.’ - Gary Bernhardt2\n\n\n\n\n\n\n# add option \nrm -R data/out",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#recap",
    "href": "dirs.html#recap",
    "title": "Directories",
    "section": "Recap",
    "text": "Recap\nGiven the principle that everything is a file in Linux, directories are treated as special types of files that contain references to other files and directories, facilitating the organization and management of data.\nUnderstanding how to navigate, manipulate, and manage directories is essential for effective use of the Linux operating system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#footnotes",
    "href": "dirs.html#footnotes",
    "title": "Directories",
    "section": "",
    "text": "The -P *.tsv option for tree tells it to look in data for files or folders with a .tsv extension. We’ll cover wildcards and patterns in the Symbols & Patterns chapter.↩︎\nAs quoted in Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools (2015) by Vince Buffalo.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "files.html",
    "href": "files.html",
    "title": "Files",
    "section": "",
    "text": "Create\nThe following commands can be used for creating, managing, and manipulating files. Some of these commands also work on directories (which we covered in the previous chapter).\nThe commands below can be used to create new files or update the time stamp of an existing file.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#create",
    "href": "files.html#create",
    "title": "Files",
    "section": "",
    "text": "touch\nWe’ll start by creating a new empty file (data/who_tb_data.tsv) with touch:\n\ntouch data/who_tb_data.tsv\n\nWe can confirm the new who_tb_data.tsv file was created, we’ll use the tree command to check the data folder:\ntree -P who_tb_data.tsv data\n# data\n# └── who_tb_data.tsv\n# \n# 1 directory, 1 file\nThe -P option lets us specify a pattern to search for in the data folder, which we’ll cover more in Symbols & Patterns.\n\n\necho\nWe can add some contents to the data/who_tb_data.tsv file using echo and the &gt; operator.1\n\necho \"country   year    type    count\nAfghanistan 1999    cases   745\nAfghanistan 1999    population  19987071\nAfghanistan 2000    cases   2666\nAfghanistan 2000    population  20595360\nBrazil  1999    cases   37737\nBrazil  1999    population  172006362\nBrazil  2000    cases   80488\nBrazil  2000    population  174504898\nChina   1999    cases   212258\nChina   1999    population  1272915272\nChina   2000    cases   213766\nChina   2000    population  1280428583\" &gt; data/who_tb_data.tsv",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#view",
    "href": "files.html#view",
    "title": "Files",
    "section": "View",
    "text": "View\ncat concatenates and displays file contents. We can use this to view the entire data/who_tb_data.tsv file we just created:\n\ncat\n\ncat data/who_tb_data.tsv\n# country   year    type    count\n# Afghanistan   1999    cases   745\n# Afghanistan   1999    population  19987071\n# Afghanistan   2000    cases   2666\n# Afghanistan   2000    population  20595360\n# Brazil    1999    cases   37737\n# Brazil    1999    population  172006362\n# Brazil    2000    cases   80488\n# Brazil    2000    population  174504898\n# China 1999    cases   212258\n# China 1999    population  1272915272\n# China 2000    cases   213766\n# China 2000    population  1280428583\n\n\n\nmore & less\nless and more lets you skim through a file on your computer, moving forwards and backwards as you please. These commands are helpful for larger files, like the Video Game Hall of Fame data stored in the data/vg_hof.tsv file:2\n\nmore data/vg_hof.tsv\n\n\n\n\nEnter ‘q’ to exit the more scroll\n\n\n\nless data/vg_hof.tsv\n\n\n\n\nEnter ‘q’ to exit the less scroll\n\n\n\n\nhead & tail\nThe head and tail commands let us view the tops and bottoms of files (the -n3 specifies three rows from data/vg_hof.tsv).\n\nhead -n3 data/vg_hof.tsv\n# year  game    developer   year_released\n# 2015  DOOM    id Software 1993\n# 2015  Pac-Man Namco   1980\n\n\ntail -n3 data/vg_hof.tsv\n# 2024  Tony Hawk's Pro Skater  Neversoft   1999\n# 2024  Ultima  Richard Garriott, Origin Systems    1981\n# 2024  You Don't Know Jack Jellyvision 1995",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#search",
    "href": "files.html#search",
    "title": "Files",
    "section": "Search",
    "text": "Search\nThese commands can help you search for files (and directories).\n\ngrep\ngrep searches files for lines matching a pattern. We’ll use it to search for a specific video game title in data/vg_hof.tsv:\n\ngrep \"The Oregon Trail\" data/vg_hof.tsv\n# 2015  The Oregon Trail    Don Rawitsch, Bill Heinemann, and Paul Dillenberger 1971\n# 2016  The Oregon Trail    Don Rawitsch, Bill Heinemann, and Paul Dillenberger 1971\n\nfind is used to search for files and directories in a directory hierarchy based on various criteria such as name, size, file type, and modification time.\n\n\nfind\nThe .psv extension is used for pipe-separated files (|). We’ll use find to locate any .psv files in data/:\n\nfind data -name \"*.psv\"\n# data/wu_tang.psv\n\nfind can be very specific, too. For example, the commands below look in the data directory for tab-delimited files (i.e., with a .tsv extension modified in the last day.\n\nfind data -name \"*.tsv\" -mtime -1\n# data/who_tb_data.tsv\n\n\n\nlocate\nlocate finds files by name quickly using a database.3\n\nlocate who_tb_data data | head -n3\n# /Users/mjfrigaard/projects/books/fm-unix/data/csv/who_tb_data.csv\n# /Users/mjfrigaard/projects/books/fm-unix/data/tsv/who_tb_data.tsv\n# /Users/mjfrigaard/projects/books/fm-unix/data/who_tb_data.csv\n\n\n\n\n\n\n\nUpdating locate database\n\n\n\n\n\n\nMake sure your locate database is up-to-date using one of the commands below if you’ve recently added or moved files to get accurate results:\n# on linux\nsudo updatedb\n# on macos\nsudo /usr/libexec/locate.updatedb",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#manage",
    "href": "files.html#manage",
    "title": "Files",
    "section": "Manage",
    "text": "Manage\nThe commands below can be used for copying, moving, renaming, and creating links to files. Assume we want to create backups of the delimiter-separated data files in the data/ folder. We’ll store these backups in folders according the file extension.\nFirst we need to create folders for each type of delimiter (.tsv and .psv):\n\nmkdir data/tsv\nmkdir data/psv\n\nConfirm these new folders with tree -d\n\ntree data -d\n# data\n# ├── psv\n# ├── raw\n# └── tsv\n# \n# 4 directories\n\n\ncp\nWe’ll copy the files into their respective folder based their extension using cp and *. For reference, here is a look of the data/ folder before and after copying the .tsv files:\n\ncp data/*.tsv data/tsv/\n\n\n\n\n\n\n\nBefore copying .csv files into data/csv/:\n\n\nAfter copying .csv files into data/csv/:4\n\n\n\n\n\n\n\n\n\ntree data -L 2 -P '*.tsv' --filesfirst\n# data\n# ├── music_vids.tsv\n# ├── pwrds.tsv\n# ├── trees.tsv\n# ├── vg_hof.tsv\n# ├── who_tb_data.tsv\n# ├── wu_tang.tsv\n# ├── psv\n# ├── raw\n# └── tsv\n# \n# 4 directories, 6 files\n\n\ntree data -L 2 -P '*.tsv' --filesfirst\n# data\n# ├── music_vids.tsv\n# ├── pwrds.tsv\n# ├── trees.tsv\n# ├── vg_hof.tsv\n# ├── who_tb_data.tsv\n# ├── wu_tang.tsv\n# ├── psv\n# ├── raw\n# └── tsv\n#     ├── music_vids.tsv\n#     ├── pwrds.tsv\n#     ├── trees.tsv\n#     ├── vg_hof.tsv\n#     ├── who_tb_data.tsv\n#     └── wu_tang.tsv\n# \n# 4 directories, 12 files\n\n\n\nNote that the number of .tsv files doubled (from six files to twelve files). We’ll do the same for the .psv files in data/\n\ncp data/*.psv data/psv/\n\nConfirm the .psv files were copied:\n\ntree data/psv/\n# data/psv/\n# └── wu_tang.psv\n# \n# 1 directory, 1 file\n\n\n\nmv\nThe commands below create a .psv version of the who_tb_data.tsv we created above:\n\ntouch data/who_tb_data.psv\necho \"| country     | year | type       | count      |\n|-------------|------|------------|------------|\n| Afghanistan | 1999 | cases      | 745        |\n| Afghanistan | 1999 | population | 19987071   |\n| Afghanistan | 2000 | cases      | 2666       |\n| Afghanistan | 2000 | population | 20595360   |\n| Brazil      | 1999 | cases      | 37737      |\n| Brazil      | 1999 | population | 172006362  |\n| Brazil      | 2000 | cases      | 80488      |\n| Brazil      | 2000 | population | 174504898  |\n| China       | 1999 | cases      | 212258     |\n| China       | 1999 | population | 1272915272 |\n| China       | 2000 | cases      | 213766     |\n| China       | 2000 | population | 1280428583 |\" &gt; data/who_tb_data.psv\n\nOops–we created the who_tb_data.psv in the data folder and not the data/psv folder: 5\n\ntree data -L 2 -P '*who_tb_data.psv'\n# data\n# ├── psv\n# ├── raw\n# ├── tsv\n# └── who_tb_data.psv\n# \n# 4 directories, 1 file\n\nWe’ll use mv to move the who_tb_data.psv and data/who_tb_data.psv files into data/psv/ and confirm with tree:6\n\nmv data/who_tb_data.psv data/psv/who_tb_data.psv\n\n\ntree data/psv\n# data/psv\n# ├── who_tb_data.psv\n# └── wu_tang.psv\n# \n# 1 directory, 2 files\n\n\n\nln\nln is used to create links between files. It can create two types of links: hard links and symbolic (or soft) links.\n\n\n\n\n\n\nA hard link is an additional name for an existing file on the same file system, and is effectively an additional directory entry for the file. In Linux file systems, all file names are technically hard links.\n\n\nA symbolic link (often called a symlink) is a file that points to another file or directory, and it contains a path to another entry somewhere in the file system.\n\n\n\nWith the ln command, you need to specify the target file first (the original file) and then the name of the new link:\n\nln original_file.txt new_link.txt\n\n\n\n\n\n\n\nln\n\n\n\n\n\n\nln doesn’t produce any output and returns zero when it’s successful.\n\n\n\n\nWe’ll use ln to create data/who_tb_data.psv, a hard link for the data file in data/psv/who_tb_data.psv:\n\nln data/psv/who_tb_data.psv data/who_tb_hardlink.psv\n\nIf we check the data folder with tree, we see the new who_tb_data.psv file looks identical to the other files:\n\ntree data -P '*.psv'\n# data\n# ├── csv\n# ├── psv\n# │   ├── who_tb_data.psv\n# │   └── wu_tang.psv\n# ├── tsv\n# ├── who_tb_hardlink.psv\n# └── wu_tang.psv\n# \n# 4 directories, 4 files\n\nHard links are basically copies–changes made to one will reflect in the other since they both refer to the same data.\nNow we’ll use ln -s to create data/who_tb_symlink.csv, a symlink for the data file in data/raw/who_tb_data.csv.\n\nln -s raw/who_tb_data.csv data/who_tb_symlink.csv\n\nWhen we look at the folder with tree now, we see the symlink is listed with a special pointer (-&gt;) to the original file:\n\ntree data -P '*.csv'\n# data\n# ├── psv\n# ├── raw\n# │   ├── music_vids.csv\n# │   ├── pwrds.csv\n# │   ├── trees.csv\n# │   ├── vg_hof.csv\n# │   ├── who_tb_data.csv\n# │   └── wu_tang.csv\n# ├── tsv\n# └── who_tb_symlink.csv -&gt; raw/who_tb_data.csv\n# \n# 4 directories, 7 files\n\nThe symbolic link only references the actual file, but doesn’t store the data itself.\n\n\n\n\n\n\nTip: ln & tree\n\n\n\n\n\n\nThe tree command displays the target file first (the original file) and the link using color:\n\n\n\nColor for symlinks with tree",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#info",
    "href": "files.html#info",
    "title": "Files",
    "section": "Info",
    "text": "Info\nThe commands below return different types of information from a files (or files).\n\nls\nThe ls command lists the contents of a directory.\n\nls data/who_tb_hardlink.psv\n# data/who_tb_hardlink.psv\n\nAdding -l returns the contents in a detailed long format. The information below is from the hardlink for the .psv file.\n\nls -l data/psv/who_tb_data.psv\n# -rw-r--r--@ 2 username  staff  686 May 16 07:47 data/psv/who_tb_data.psv\n\n-l will add information like file permissions, number of links, owner, group, file size, and last modification date for each item.\nIf we check the hardlink for the .psv file:\n\nls -l data/who_tb_hardlink.psv\n# -rw-r--r--@ 2 username  staff  686 May 16 07:47 data/who_tb_hardlink.psv\n\nBoth ls -l commands return similar information for the who_tb_data.psv files, indicating they’re not treated as separate files, but rather two paths to the same file.\nCompare this to the symlink we created with it’s original file:\n\nls -l data/who_tb_symlink.csv\n# lrwxr-xr-x@ 1 username  staff  19 May 16 07:50 data/who_tb_symlink.csv -&gt; raw/who_tb_data.csv\n\nThe ls -l output for data/who_tb_data.csv (the symlink) returns lrwxr-xr-x (file permissions for a symbolic link) and\n\nls -l data/raw/who_tb_data.csv\n# -rw-r--r--@ 1 uername  staff  381 May 15 21:55 data/raw/who_tb_data.csv\n\nThe who_tb_data.csv file in data/raw returns -rw-r--r-- (standard file permissions),\nFile link info (@)\nThe preceding @ 2 from ls -l data/who_tb_hardlink.psv indicates the files have two hard links, meaning who_tb_data.psv is physically located in one place but can be accessed from two locations: data/who_tb_data.psv and data/psv/who_tb_data.psv. The @ 1 from ls -l data/csv/who_tb_data.csv indicates a single hard link (symbolic links always have one link).\nWe’ll cover this output more in the Permissions section below.\n\n\ndiff\ndiff compare the contents of two files line-by-line. We’ll use diff to compare the pipe-separated values file (data/wu_tang.psv) to the comma-separated separated values file (data/wu_tang.csv)\n\ndiff data/wu_tang.psv data/wu_tang.csv\n\n\n\n\n\n\n\n# 1,11c1,11\n\n\nThe first line of the output indicates that lines 1 through 11 in the first file (data/wu_tang.psv) have been changed compared to lines 1 through 11 in the second file (data/wu_tang.csv).\n\n\n\n\n\n\n\n\n\nLines starting with &lt; indicate the content from the first file (data/wu_tang.psv). These entries are separated by pipes or spaces (as commonly used in PSV files).\n\n\n# &lt; |Member           |Name                 |\n# &lt; |RZA              |Robert Diggs         |\n# &lt; |GZA              |Gary Grice           |\n# &lt; |Method Man       |Clifford Smith       |\n# &lt; |Raekwon the Chef |Corey Woods          |\n# &lt; |Ghostface Killah |Dennis Coles         |\n# &lt; |Inspectah Deck   |Jason Hunter         |\n# &lt; |U-God            |Lamont Hawkins       |\n# &lt; |Masta Killa      |Jamel Irief          |\n# &lt; |Cappadonna       |Darryl Hill          |\n# &lt; |Ol Dirty Bastard |Russell Tyrone Jones |\n\n\n\n\n\n\n\n\n\n# &gt; Member,Name\n# &gt; RZA,Robert Diggs\n# &gt; GZA,Gary Grice\n# &gt; Method Man,Clifford Smith\n# &gt; Raekwon the Chef,Corey Woods\n# &gt; Ghostface Killah,Dennis Coles\n# &gt; Inspectah Deck,Jason Hunter\n# &gt; U-God,Lamont Hawkins\n# &gt; Masta Killa,Jamel Irief\n# &gt; Cappadonna,Darryl Hill\n# &gt; Ol Dirty Bastard,Russell Tyrone Jones\n\n\nLines starting with &gt; show the content from the second file (data/wu_tang.csv). These entries are separated by commas, as is typical for CSV files.\n\n\n\nThere is no difference in the actual data (Member or Name)–both files contain the same information, so the primary difference is purely in the formatting of the data: the PSV (pipe-separated Values) file uses vertical bars (|) and spaces to separate data fields, whereas the CSV (comma-separated values) file uses commas (,).7\nWhat happens when we compare the symlink (data/who_tb_data.csv) and it’s original file (data/raw/who_tb_data.csv) with diff?\n\ndiff data/who_tb_symlink.csv data/raw/who_tb_data.csv\n\ndiff returns nothing and doesn’t produce any output if there are no differences between the two files.\nDeleting, moving, or renaming the original file does not affect the integrity of a hard link:\n\n# remove original file\nrm data/psv/who_tb_data.psv\n# check hard link\ncat data/who_tb_hardlink.psv\n# | country     | year | type       | count      |\n# |-------------|------|------------|------------|\n# | Afghanistan | 1999 | cases      | 745        |\n# | Afghanistan | 1999 | population | 19987071   |\n# | Afghanistan | 2000 | cases      | 2666       |\n# | Afghanistan | 2000 | population | 20595360   |\n# | Brazil      | 1999 | cases      | 37737      |\n# | Brazil      | 1999 | population | 172006362  |\n# | Brazil      | 2000 | cases      | 80488      |\n# | Brazil      | 2000 | population | 174504898  |\n# | China       | 1999 | cases      | 212258     |\n# | China       | 1999 | population | 1272915272 |\n# | China       | 2000 | cases      | 213766     |\n# | China       | 2000 | population | 1280428583 |\n\nHowever, if the original file for a symlink is deleted, moved, or renamed, the symbolic link breaks and typically becomes a ‘dangling’ link that points to a non-existent path.\n\n\nfile\nfile gives us a summary of what a file is or what it contains, like telling us what’s in data/who_tb_data.csv.\n\nfile data/who_tb_symlink.csv\n# data/who_tb_symlink.csv: CSV text\n\nThe -i option will tell us if this is a regular file:\n\nfile -i data/who_tb_symlink.csv\n# data/who_tb_symlink.csv: regular file\n\n\n\nreadlink\nreadlink displays the target of a symbolic link.\n\nreadlink data/who_tb_symlink.csv\n# raw/who_tb_data.csv\n\nThe -f option provides the target’s absolute path.\n\nreadlink -f data/who_tb_symlink.csv\n# path/to/data/raw/who_tb_data.csv\n\n\n\nwc\nwc (word count) counts the number of lines, words, and characters in the given input. If a file name is provided, it performs the count on the file; otherwise, it reads from the standard input.\n\nwc data/who_tb_symlink.csv\n#       13      13     381 data/who_tb_symlink.csv\n\n\n\nstat\nstat displays detailed information about files.\n\nstat data/who_tb_symlink.csv\n# 16777221 317774438 lrwxr-xr-x 1 username staff 0 19 \"May 13 13:10:34 2024\" \\\n#     \"May 13 13:10:34 2024\" \"May 13 13:10:34 2024\" \"May 13 13:10:34 2024\"     \\ \n#     4096 0 0 data/who_tb_symlink.csv\n\nAdding the -l includes the symbolic link to the original file.\n\nstat -l data/who_tb_symlink.csv\n# lrwxr-xr-x 1 username staff 19 May 13 13:10:34 2024 \\\n#     data/who_tb_symlink.csv -&gt; csv/who_tb_data.csv\n\n\n\ndu\ndu estimates file space usage.\n\ndu data/raw/pwrds.csv\n# 24    data/raw/pwrds.csv\n\nThe -h makes the output human readable.\n\ndu -h data/raw/pwrds.csv\n#  12K  data/raw/pwrds.csv\n\nIf we pass the original file and symlink of who_tb_data.csv to du, we see the symlink doesn’t contain any actual data:\n\ndu -h data/raw/who_tb_data.csv\n# 4.0K  data/raw/who_tb_data.csv\n\n\ndu -h data/who_tb_symlink.csv\n#   0B  data/who_tb_symlink.csv",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#sec-file-permissions",
    "href": "files.html#sec-file-permissions",
    "title": "Files",
    "section": "Permissions",
    "text": "Permissions\nWe’ll go over file permissions in-depth in the Permissions chapter, but I’ll quickly summarize two common uses of chmod below. The file permissions are printed with the ls -l output:\nls -l data/README.md\n# -rw-r--r--@ 1 username  staff  8834 May 14 09:33 data/README.md\n\nchmod\nchmod changes file permissions. To change the file permissions using chmod, using either Below are some simple examples using symbolic notation.8\nTo grant the permissions above (i.e., -rw-r--r--) using symbolic notation with chmod, we could use:\nchmod u=rw,g=r,o=r data/README.md\nu=rw: sets the user (owner) permissions to read and write.\ng=r: sets the group permissions to read.\no=r: set the permissions for others to read.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#recap",
    "href": "files.html#recap",
    "title": "Files",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#footnotes",
    "href": "files.html#footnotes",
    "title": "Files",
    "section": "",
    "text": "data/who_tb_data.tsv comes from the WHO global tuberculosis programme.↩︎\ndata/vg_hof.tsv is the Video Game Hall of Fame data↩︎\nlocate sometimes requires the search database is generated/updated. Read more here↩︎\nThe -L 2 option tells tree to only look in the data folder (no subfolders), -P '*.tsv' matches the .csv files, and --filesfirst lists the files first, then the new directories.↩︎\nThe -L 2 option tells tree to only look in the data folder (no subfolders) and -P '*who_tb_data.psv' matches the who_tb_data.psv file.↩︎\ncp and mv also work with directories.↩︎\nThis type of difference is significant if the format impacts how data is parsed or used. For example, a software program expecting data in CSV format might not correctly parse a PSV file, and vice versa.↩︎\nWe’ll cover octal (numeric) notation in the Permissions chapter.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "sys.html",
    "href": "sys.html",
    "title": "System",
    "section": "",
    "text": "Real-time monitoring\nThis chapter covers monitoring performance and resource usage in the Linux systems. These commands will help ensure that the servers and applications are running efficiently and can help you preemptively address potential issues before they escalate into major problems.\nWe’ll delve into some of the most essential and widely-used system monitoring commands: top, htop, free, df, and du",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "sys.html#real-time-monitoring",
    "href": "sys.html#real-time-monitoring",
    "title": "System",
    "section": "",
    "text": "top\ntop displays a dynamic, real-time view of the system’s processes, including CPU and memory usage.\n\ntop\n\n\n\n\ntop in iTerm2\n\n\n\n\nhtop\nhtop is a more user-friendly and visually appealing alternative to top.1\n\n\n\nhtop in GNOME\n\n\nhtop enhances usability with features like color coding, an interactive interface, and the ability to scroll through processes.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "sys.html#snapshots",
    "href": "sys.html#snapshots",
    "title": "System",
    "section": "Snapshots",
    "text": "Snapshots\n\nfree\nfree displays the amount of free and used memory in the system, giving you a quick snapshot of memory allocation and availability:\n\n\n\nBash  on Linux .\n\n\nYou can add -h to make it more human-readable:\n\n\n\ndf\ndf (disk free) shows disk usage in a human-readable format, including the size, used space, available space, and the mount point of each filesystem.\n\ndf\n#&gt; Filesystem                       512-blocks      Used Available Capacity iused     ifree %iused  Mounted on\n#&gt; /dev/disk1s1s1                    976490576  20002808  84798224    20%  403755 423991120    0%   /\n#&gt; devfs                                   395       395         0   100%     684         0  100%   /dev\n#&gt; /dev/disk1s3                      976490576   5059568  84798224     6%    5077 423991120    0%   /System/Volumes/Preboot\n#&gt; /dev/disk1s5                      976490576   2097312  84798224     3%       1 423991120    0%   /System/Volumes/VM\n#&gt; /dev/disk1s6                      976490576     39200  84798224     1%      19 423991120    0%   /System/Volumes/Update\n#&gt; /dev/disk1s2                      976490576 861746736  84798224    92% 5662798 423991120    1%   /System/Volumes/Data\n#&gt; map auto_home                             0         0         0   100%       0         0     -   /System/Volumes/Data/home\n\nBy default, it displays sizes in 1K blocks but can show them in a more readable format (like MB or GB) with the -h option (human-readable).\n\ndf -h\n# Filesystem                          Size    Used   Avail Capacity iused ifree %iused  Mounted on\n# /dev/disk1s1s1                     466Gi   9.5Gi    35Gi    22%    404k  367M    0%   /\n# devfs                              197Ki   197Ki     0Bi   100%     682     0  100%   /dev\n# /dev/disk1s3                       466Gi   2.4Gi    35Gi     7%    5.1k  367M    0%   /System/Volumes/Preboot\n# /dev/disk1s5                       466Gi   2.0Gi    35Gi     6%       2  367M    0%   /System/Volumes/VM\n# /dev/disk1s6                       466Gi    20Mi    35Gi     1%      19  367M    0%   /System/Volumes/Update\n# /dev/disk1s2                       466Gi   415Gi    35Gi    93%    6.2M  367M    2%   /System/Volumes/Data\n# map auto_home                        0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home\n\n\n\ndu\ndu summarizes disk usage for directories and files, allowing you to pinpoint large files and directories that may be consuming excessive disk space.\n\ndu data\n# 128   data/raw\n# 240   data\n\nJust like df, show the output in a more readable format (like MB or GB) with the -h (human-readable) option:\n\ndu -h data\n#  64K  data/raw\n# 120K  data\n\n\n\nps\nps reports a snapshot of the process status for all running processes, regardless of the owner, including the process ID (PID), terminal type (TTY), cumulative CPU time (TIME), and the command (CMD) that started each process:\n\nps\n#   PID TTY           TIME CMD\n# 10409 ttys000    0:00.01 -bash\n\nCommand options can also expand the selection to include other users’ processes, full command lines, etc.\n\nps -o lstart,sess,pmem,pcpu,etime,command\n# STARTED                        SESS %MEM  %CPU ELAPSED COMMAND\n# Fri Aug 30 13:08:56 2024          0  0.0   0.0   38:09 -bash\n\nThe specified columns are:\n\nlstart: The exact date and time the process was started\n\nsess: The session ID of the process\n\npmem: The percentage of the physical memory used by the process\n\npcpu: The percentage of the CPU time used by the process\n\netime: The elapsed time since the process started\ncommand: The command and arguments",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "sys.html#recap",
    "href": "sys.html#recap",
    "title": "System",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we explored five essential system monitoring commands in Linux: top, htop, free, df, and du. These commands provide crucial insights into system performance and resource utilization, enabling efficient monitoring and troubleshooting of Linux systems.\nMastering these commands will help you monitor and maintain optimal system performance, ensure efficient resource utilization, and quickly address potential issues.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "sys.html#footnotes",
    "href": "sys.html#footnotes",
    "title": "System",
    "section": "",
    "text": "You can install htop on macOS with homebrew. Read more here↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "syntax.html",
    "href": "syntax.html",
    "title": "Syntax",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is being revised. Thank you for your patience.\n\n\n\n\nIn the previous section we covered some of the basic common commands you’ll use in a Linux-like operating systems. This section dives deeper into the syntax–the commands, arguments, and options–you’ll type into the terminal or write in a shell script. We’ll also discuss creating variables and how to chain commands together using pipes\n\nCommands\n\n\nArguments\n\n\nOptions\n\n\nPipes\n\n\nVariables\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax"
    ]
  },
  {
    "objectID": "commands.html",
    "href": "commands.html",
    "title": "Commands",
    "section": "",
    "text": "REPL\nCommands are the first part of a command line instruction, specifying the program or built-in functionality to be executed. In Unix/Linux, several commands can operate without any options or arguments, performing their basic functions in their simplest form.\nWe’re going to start with a basic command: date:\nThe command date returns the current date and time. What we’ve just done is referred to as the read–eval–print loop, or REPL, and it’s the underlying process of the command-line.\nThe REPL in Bash exemplifies a powerful and flexible interface for interacting with the system, running commands, and developing scripts, providing both novice and experienced users with an efficient way to manage their computing environment.\nHere is how it works:",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#repl",
    "href": "commands.html#repl",
    "title": "Commands",
    "section": "",
    "text": "1. Read\nIn the Bash, the “Read” step occurs when the shell waits for input from the user. This is typically represented by the shell prompt, where we can type commands.\nThe prompt might display useful information, such as the current user (username), hostname (hostname), and working directory (current_directory), depending on its configuration.\n\n  user@host:directory$\n\n\n\n\n2. Eval\nOnce a command is entered, Bash “evaluates” it. This step involves parsing the command and its arguments, checking for syntax correctness, and then executing it.\n\n  user@host:directory$ date\n\n\nCommands can be simple, such as listing the current date and time, or complex scripts involving loops, conditionals, and functions.\n\n\n3. Print\nAfter evaluating the command, Bash “prints” the output or the result of the command execution to the screen (stdout or standard output) or another specified location.\n\n  # Wed Apr 10 02:55:23 MST 2024\n\n\n\n\nLoop\nAfter executing a command and returning the output, Bash immediately returns to the “read” step, displaying the prompt and waiting for new user input.\n\nuser@host:directory$\nuser@host:directory$ date\n#&gt; Wed Apr 10 02:55:23 MST 2024\nuser@host:directory$\n\nThis cycle repeats indefinitely until the user exits the REPL environment, typically with an exit command or by pressing Ctrl + D.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#commands",
    "href": "commands.html#commands",
    "title": "Commands",
    "section": "Commands",
    "text": "Commands\nIn Unix, several commands can operate without any options or arguments, performing their basic functions in their simplest form. Below are some of these commands:\n\nwho\nwho shows who is logged on the system.\n\nwho\n# username       console      Apr 14 13:45 \n# username       ttys000      Apr 14 13:45 \n# username       ttys001      Apr 14 13:45 \n# username       ttys003      Apr 16 05:56\n\nwho by itself, without options or arguments, lists the users currently logged into the system.\n\n\nwhoami\nwhoami shows the username of the user currently logged into the system.\n\nwhoami\n# username\n\n\n\nhostname\nhostname displays the system’s network name.\n\nhostname\n# Users-MacBook-Pro.local\n\n\n\ncal\ncal displays a calender of the current month.\n\ncal\n#      April 2024       \n# Su Mo Tu We Th Fr Sa  \n#     1  2  3  4  5  6  \n#  7  8  9 10 11 12 13  \n# 14 15 16 17 18 19 20  \n# 21 22 23 24 25 26 27  \n# 28 29 30              \n# \n\n\n\nuptime\nuptime shows how long the system has been running.\n\nuptime\n# 11:39  up 15:05, 4 users, load averages: 3.85 3.08 2.93\n\n\n\nclear\nclear clears the terminal screen and doesn’t print any return values.\n\nclear\n\nclear does its job without the need for additional input.\n\n\nexit\nexit exits the shell or your current session.\n\nexit\n\nexit requires no arguments or options to execute this action, and doesn’t print any return values.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#recap",
    "href": "commands.html#recap",
    "title": "Commands",
    "section": "Recap",
    "text": "Recap\nEach of these commands performs a specific and often utilized function within the Unix environment, embodying the Unix philosophy of doing one thing well.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "arguments.html",
    "href": "arguments.html",
    "title": "Arguments",
    "section": "",
    "text": "Anatomy\nArguments in Unix/Linux commands are values or data passed to commands for processing. Unlike options, which modify the behavior of commands, arguments typically specify what the command should act upon. This typically includes filenames, user names, data values, or other kinds of information the command needs to execute its task.\nA Unix command can be broken down into the command name, followed by its options (which we’ll address in the next chapter), and then its arguments:\ncommand argument1 argument2 ... argument",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "arguments.html#argument-types",
    "href": "arguments.html#argument-types",
    "title": "Arguments",
    "section": "Argument Types",
    "text": "Argument Types\nBelow are a variety of command arguments types. This is not an exhaustive list, but includes many of the commands and arguments you’ll encounter on a regular basis.\n\nDirect arguments\nDirect arguments are the most straightforward type of arguments. They are typically the names of files or directories on which commands operate.\nExample\nIn the command cat my_file.txt, my_file.txt is a direct argument to the cat command, telling it which file to display on the standard output.\n\ncat my_file.txt\n# This is my file\n\n\n\nIndirect arguments\nIndirect arguments are arguments that might specify additional information that commands need to complete their tasks.\nExample\nThe file search pattern for the grep command is an example of an indirect command, and myfile.txt is the direct argument.\n\ngrep file my_file.txt\n# This is my file\n\nMost commonly, arguments are the names of files and directory names on which the command will operate.\nExample\nmy_file.txt and tmp/tmp_file.txt are arguments representing the source and destination locations to move with mv, respectively.\n\nmv my_file.txt tmp/tmp_file.txt\n\nCommands related to user management might take user and group names names as arguments.\nExample\nchown changes the ownership of file to user and group.\n\nchown user:group myfile.txt\n\n\n\nCommand Targets\nSome commands take other commands as arguments.\nExample\nFor example, sudo command runs command with superuser privileges.\n\nsudo vi path/to/file.config\n\n\n\nData Values\nCommands might take data values as arguments for processing.\nExample\nIn echo Hello, World!, Hello, World! is the argument value that echo prints to the terminal.\n\necho Hello, World!\n# Hello, World!\n\n\n\nOrder and Position\nFor many commands, the order of the arguments is significant.\nExample\ntmp/tmp_file.txt is the first argument (indicating the file to copy from), and new_file.txt is the second argument (indicating where to copy the file to).\n\ncp tmp/tmp_file.txt new_file.txt\n\nReversing these arguments would result in a completely different operation.\nArguments that contain spaces must be quoted or escaped, so the shell understands them as a single argument rather than multiple arguments.\nExample\nTo copy the contents of the new new_file.txt to 'my new file.txt', you would use:\n\ncp new_file.txt 'my new file.txt'\n\n\n\nCommand Substitution\nThe output of a command can be used as an argument for another command using backticks (` `) or $( ).\nExample\necho $(grep file 'my file 2.txt') uses the output of the grep command as an argument for echo:\n\necho $(grep file 'my new file.txt')\n# This is my file\n\n\n\nVariables as Arguments\nEnvironment variables can be used as arguments in commands.\nExample\necho $HOME prints the path to the user’s home directory, where $HOME is an argument that the echo command interprets:\n\necho $HOME\n# /Users/username",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "arguments.html#recap",
    "href": "arguments.html#recap",
    "title": "Arguments",
    "section": "Recap",
    "text": "Recap\nUnderstanding the nuances of Unix arguments is crucial for crafting precise and effective commands, allowing users to leverage the full power of the Unix command line for a wide array of tasks.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "options.html",
    "href": "options.html",
    "title": "Options",
    "section": "",
    "text": "Option types\nUnix command options (sometimes called flags) are often preceded by a hyphen (-) or two (-- for long-form options) and modify the behavior of a command. Options provide flexibility, control, and, in some cases, enable a vast array of additional functionalities.\nArguments can be short or long:",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#option-types",
    "href": "options.html#option-types",
    "title": "Options",
    "section": "",
    "text": "Short options are typically a single dash followed by a single letter (e.g., -l) and they modify the command behavior in a specific, often concise way.\n\n\nLong options usually use two dashes followed by a word or compound words (e.g., --long-listing) and provide a more descriptive way to specify options, making scripts and commands more readable.\n\n\n\n\nShort options\nExample\nls -l data lists files in data in a long format, showing detailed information like permissions, owner, size, and modification date:\n\nls -l data\n# total 112\n# -rw-r--r--@  1 mjfrigaard  staff   8798 May 15 21:59 README.md\n# -rw-r--r--@  1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--@  1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# drwxr-xr-x@ 10 mjfrigaard  staff    320 May 15 21:55 raw\n# -rw-r--r--@  1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--   1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--@  1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.psv\n# -rw-r--r--@  1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n\n\n\nLong options\nExample 1\ntree data --sort size lists files in order according to size:\n\ntree data --sort size\n# ├── ajperlis_epigrams.txt\n# ├── pwrds.csv\n# ├── pwrds.tsv\n# ├── README.md\n# ├── music_vids.tsv\n# ├── vg_hof.tsv\n# ├── trees.tsv\n# ├── roxanne.txt\n# ├── wu_tang.dat\n# ├── who_tb_data.tsv\n# ├── who_tb_data.txt\n# ├── wu_tang.txt\n# ├── wu_tang.csv\n# ├── wu_tang.tsv\n# ├── roxanne_rev.txt\n# └── roxanne_orig.txt\n# \n# 1 directory, 16 files\n\nExample 2\ngrep --help displays usage information for the grep command, helping users understand available options and syntax.\n\ngrep --help\n#&gt; usage: grep [-abcdDEFGHhIiJLlMmnOopqRSsUVvwXxZz] [-A num] [-B num] [-C[num]]\n#&gt;  [-e pattern] [-f file] [--binary-files=value] [--color=when]\n#&gt;  [--context[=num]] [--directories=action] [--label] [--line-buffered]\n#&gt;  [--null] [pattern] [file ...]\n\nExample 3\nThe --version option is commonly used to get version information for various commands.\n\ngrep --version  \n# grep (BSD grep, GNU compatible) 2.6.0-FreeBSD\n\nNote that not all commands support the long-form option syntax.\n\n\nModifying behavior\nSome options modify the commands behavior.\nExample\ncp-n source.txt dest.txt does not overwrite the destination file if it already exists.\n\ncat myfile.txt\n# This is my file\n\n\ncp -n myfile.txt myfile2.txt\n\nThe -n option modifies the default behavior of the cp command.\n\ncat myfile2.txt\n# This is my 2nd file\n\n\n\nModifying output\nOther options modify a commands output.\nExample 1\nls -a lists all files, including hidden ones (those starting with a dot). This option alters the command’s output by showing files that are not listed by default.\n\nls -a data\n# .\n# ..\n# README.md\n# music_vids.tsv\n# pwrds.tsv\n# raw\n# trees.tsv\n# vg_hof.tsv\n# wu_tang.psv\n# wu_tang.tsv\n\nExample 2\ndf -h shows disk space usage in human-readable form (e.g., KB, MB, GB), modifying the default output to be more easily understood.\n\ndf -h\n# Filesystem                          Size    Used   Avail Capacity iused ifree %iused  Mounted on\n# /dev/disk1s1s1                     466Gi   9.5Gi    40Gi    20%    404k  422M    0%   /\n# devfs                              201Ki   201Ki     0Bi   100%     696     0  100%   /dev\n# /dev/disk1s3                       466Gi   2.4Gi    40Gi     6%    5.1k  422M    0%   /System/Volumes/Preboot\n# /dev/disk1s5                       466Gi   1.0Gi    40Gi     3%       1  422M    0%   /System/Volumes/VM\n# /dev/disk1s6                       466Gi    19Mi    40Gi     1%      19  422M    0%   /System/Volumes/Update\n# /dev/disk1s2                       466Gi   411Gi    40Gi    92%    6.3M  422M    1%   /System/Volumes/Data\n# map auto_home                        0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home\n\n\n\nEnvironment-specific options\nThe -u option in sort removes duplicate lines. -us behavior (considering case sensitivity) might vary depending on the locale and environment settings.\nExample\nsort -u data/roxanne.txt sorts the lines in data/roxanne.txt, removing duplicate lines.1\n\nsort -u data/raw/roxanne.txt\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#combining-options",
    "href": "options.html#combining-options",
    "title": "Options",
    "section": "Combining Options",
    "text": "Combining Options\nMultiple short options can be combined after a single dash, without spaces (e.g., -lrt) allowing users to use multiple options at once, and reducing the need to type multiple dashes.\n\nMultiple short options\nOptions can be combined with arguments when they are followed by a space and then the argument (e.g., -o filename).\nExample\nls -lrt combines three options: -l (long listing format), -r (reverse order), and -t (sort by modification time), providing a detailed, reverse-chronological list of files.\n\nls -lrt data\n# total 112\n# -rw-r--r--@  1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@  1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--@  1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--   1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--@  1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.psv\n# -rw-r--r--@  1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# drwxr-xr-x@ 10 mjfrigaard  staff    320 May 15 21:55 raw\n# -rw-r--r--@  1 mjfrigaard  staff   8798 May 15 21:59 README.md\n\n\n\nOptions with values\nSome options require or accept an argument to specify a value related to the option’s action.\nExample\ngrep-i \"FILE\" myfile.txt uses -i to ignore case when searching for \"FILE\" in myfile.txt:\n\ngrep -i \"FILE\" myfile.txt\n# This is my file\n\nThe \"FILE\" here is an argument for the -i option.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#recap",
    "href": "options.html#recap",
    "title": "Options",
    "section": "Recap",
    "text": "Recap\nOptions greatly enhance the power and versatility of Unix commands, allowing users to tailor operations to their specific needs and preferences.\nNot all Unix-like systems or shells may support the same options for a given command, and behavior can vary between implementations. It’s important to refer to a command’s manual page (using man command or command --help) for the most accurate and comprehensive list of options and their effects.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.\n\n\n\n\n\nrm myfile.txt\nrm myfile2.txt",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#footnotes",
    "href": "options.html#footnotes",
    "title": "Options",
    "section": "",
    "text": "data/roxanne.txt contains the lyrics to the 1978 song Roxanne by The Police.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "pipes.html",
    "href": "pipes.html",
    "title": "Pipes",
    "section": "",
    "text": "Fundamental Concept\nThe Unix pipe, denoted by the vertical bar |, is a powerful feature of Unix and Unix-like operating systems that allows the output of one command (stdout) to be used as the input to another (stdin). This capability forms the basis of the Unix philosophy of building small, modular utilities that do one thing well and connecting them together to perform complex tasks.\nThe pipe is placed between two commands and directs the standard output (stdout) of the command to the left of the pipe to the standard input (stdin) of the command to the right.\nExample\necho \"Hello, World!\" | wc -w sends the output of the echo command to wc, which then counts the words.\necho \"Hello, World!\" | wc -w\n#        2\nThe output is 2, indicating there are two words in “Hello, World!”.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#fundamental-concept",
    "href": "pipes.html#fundamental-concept",
    "title": "Pipes",
    "section": "",
    "text": "Standard Input and Output\n\n\n\n\n\n\n\n\nstdin (standard input) is a text stream from which a command reads its input. By default, it’s the keyboard, but it can be redirected to read from a file or another command’s output.\nstdout (standard output) is a text stream where a command writes its output. Typically, this is the terminal screen, but it can be redirected to a file or another command’s input.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#combining-pipes",
    "href": "pipes.html#combining-pipes",
    "title": "Pipes",
    "section": "Combining Pipes",
    "text": "Combining Pipes\nCommands can be chained together using multiple pipes, allowing for the creation of command pipelines where data is processed in stages.\nExample\npsaux |grep httpd lists all processes, filters those containing “httpd” (HTTPD = web server processes running):\n\nps aux | grep httpd\n# mjfrigaard       23601   0.0  0.0 33597016    632   ??  S     6:44AM   0:00.00 grep httpd\n# mjfrigaard       23599   0.0  0.0 33599596    928   ??  S     6:44AM   0:00.01 bash -c ps aux | grep httpd\n# mjfrigaard       23598   0.0  0.0 33598572    932   ??  S     6:44AM   0:00.01 sh -c 'bash'  -c 'ps aux | grep httpd' 2&gt;&1\n\nExample\nwc-l counts the number of lines:\n\nps aux | grep httpd | wc -l\n#        3\n\n\nFiltering and Processing\nExample 1\ncatdata/roxanne.txt |grep\"night\" displays lines from data/roxanne.txt that contain the number \"2\".\n\ncat data/raw/roxanne.txt | grep \"night\"\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\nHere, cat outputs the file’s contents, which grep filters.\nExample 2\nls-l data/raw |sort-r lists the files in data/raw in a detailed format, then sorts them in reverse order.\n\nls -l data/raw | sort -r\n# total 128\n# -rw-r--r--@ 1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   6452 May 15 21:48 music_vids.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   4828 May 15 21:49 vg_hof.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   4462 May 15 21:48 trees.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    381 May 15 21:55 who_tb_data.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n\nIt showcases how to reverse the listing of directory contents.\n\n\nTransformation and Reduction\nExample\nfind. -type f |xargsdu -sh |sort-h finds files (-type f) in the current directory and subdirectories, calculates their sizes (du -sh), and sorts them by size (sort -h):\n\nfind data -type f | xargs du -sh | sort -h\n# 4.0K  data/raw/roxanne.txt\n# 4.0K  data/raw/who_tb_data.csv\n# 4.0K  data/raw/wu_tang.csv\n# 4.0K  data/wu_tang.psv\n# 4.0K  data/wu_tang.tsv\n# 8.0K  data/music_vids.tsv\n# 8.0K  data/raw/music_vids.csv\n# 8.0K  data/raw/trees.csv\n# 8.0K  data/raw/vg_hof.csv\n# 8.0K  data/trees.tsv\n# 8.0K  data/vg_hof.tsv\n#  12K  data/README.md\n#  12K  data/pwrds.tsv\n#  12K  data/raw/pwrds.csv\n#  16K  data/raw/ajperlis_epigrams.txt\n\nThis pipeline not only identifies files but also sorts them by their disk usage, illustrating a complex operation made simple through pipes.\n\n\nReal-time Streaming and Monitoring\nExample\ncat /var/log/system.log | grep DEAD_PROCESS prints the system.log file, continuously monitoring for new entries, filters for those containing DEAD_PROCESS, then counts the number of lines:1\n\ncat /var/log/system.log | grep \"DEAD_PROCESS\" \n## Apr 10 06:35:23 Users-MacBook-Pro login[3596]: DEAD_PROCESS: 3596 ttys000\n## Apr 10 06:35:25 Users-MacBook-Pro sessionlogoutd[19895]: DEAD_PROCESS: 225 console\n## Apr 10 10:20:25 Users-MacBook-Pro login[715]: DEAD_PROCESS: 715 ttys000\n\n\n\nData Manipulation\nExample\ncut -d':' -f1 data/roxanne.txt | sort | uniq extracts the first field from each line in data/roxanne.txt, sorts the contents alphabetically, and removes duplicates.\n\ncut -d':' -f1 data/raw/roxanne.txt | sort | uniq\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\nThis sequence is an example of performing data extraction and deduplication.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#pipes-with-loops",
    "href": "pipes.html#pipes-with-loops",
    "title": "Pipes",
    "section": "Pipes with Loops",
    "text": "Pipes with Loops\nThe example below demonstrates how to use the while loop with pipes with find, echo, grep, and wc.\n\nFilter with find\nfind data -name \"*.tsv\" starts in the data directory, looking for all files that end with the .tsv extension. The search is recursive, meaning it includes all subdirectories of data as well. Produces a list of paths to .tsv files, each path on a new line. This list is piped to the next command.\n\nfind data -name \"*.tsv\" \n# data/pwrds.tsv\n# data/music_vids.tsv\n# data/vg_hof.tsv\n# data/trees.tsv\n# data/wu_tang.tsv\n\n\n\nIterate with while and do\n| while read fname; do: The pipe (|) feeds the output from the find command into a while loop, which reads each line (file name) into the variable fname, one at a time. For each iteration of the loop (i.e., for each file name read into fname), the commands within the do ... done block are executed.\n\nfind data -name \"*.tsv\" | while read fname; do\n  # do this!\ndone\n\n\n\nPrint with echo\necho -n \"$fname: \": Prints the current file’s name being processed. echo-n outputs the value of fname (the path to the current .tsv file) followed by a colon and a space, without adding a newline at the end. This means the count returned by wc will be printed on the same line, right after the file name.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/wu_tang.tsv:\n\n\n\nSearch with grep\ngrep \"RZA\" \"$fname\": Searches for a specific pattern within the file. grep looks through the contents of the file (whose path is in fname) for lines containing the string “RZA”. Only the lines that match this pattern are printed to stdout, which is then piped to wc.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/wu_tang.tsv: RZA   Robert Diggs\n\n\n\nCount with wc\nwc: For each file processed by the loop, wc outputs three numbers: the line count, word count, and character/byte count of the lines that grep found to contain “RZA”. Since no specific option is given to wc, it defaults to displaying all three counts.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\" | wc \ndone\n# data/pwrds.tsv:        0       0       0\n# data/music_vids.tsv:        0       0       0\n# data/vg_hof.tsv:        0       0       0\n# data/trees.tsv:        0       0       0\n# data/wu_tang.tsv:        1       3      17\n\nThis Bash command sequence combines find, a while loop, echo, grep, and wc to search through .tsv (Tab-Separated Values) files for lines containing a specific pattern (“RZA”) and reports the count of lines, words, and characters for each occurrence. Combining pipelines with loops is an efficient way to sift through a potentially large set of files within a directory, facilitating a detailed aggregation of specified conditions across multiple files.\n\n\n\n\n\n\nConsiderations when using pipes\n\n\n\n\n\n\nEfficiency and Performance\nWhile pipes are incredibly powerful, their use can impact performance, especially when processing large amounts of data. Each pipe involves creating a new subprocess, and data is copied between processes, which can lead to overhead.\nError Handling\nError handling in pipes can be non-trivial, as each command in a pipeline executes independently. Users need to consider how each command handles errors and ensure that the pipeline as a whole behaves as expected even when errors occur.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#recap",
    "href": "pipes.html#recap",
    "title": "Pipes",
    "section": "Recap",
    "text": "Recap\nPipes (|) allow the output of one command (stdout) to be used as the input (stdin) to another, enabling the chaining of commands to perform complex tasks with the output of one serving as the input for the next. Unix pipes embody the concept of composability in Unix, enabling users to build complex workflows out of simple, single-purpose programs. They are a testament to the flexibility and power of the Unix command line, facilitating a wide range of tasks from simple text processing to sophisticated data analysis and system monitoring.\nThis framework of commands, arguments, options, and the interplay of input (stdin), output (stdout) , and pipes enables sophisticated data processing and manipulation directly from the terminal.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#footnotes",
    "href": "pipes.html#footnotes",
    "title": "Pipes",
    "section": "",
    "text": "tail -f /var/log/syslog | grep sshd is useful for real-time monitoring of SSH daemon logs.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "vars.html",
    "href": "vars.html",
    "title": "Variables",
    "section": "",
    "text": "Unix/Linux Variables\nThis chapter covers the essential aspects of variables in Unix/Linux environments. I’ll be using VS Code because I can quickly switch between shells in the Terminal window.\nVariables in Unix/Linux are used to store data that can be accessed and manipulated by the operating system and applications. They play a crucial role in scripting, programming, and managing system configurations. By storing values in variables, users and scripts can dynamically reference and change these values without hard-coding them, making scripts more flexible and maintainable.\nIn Unix/Linux, variables can hold different types of data, such as strings, integers, or file paths. They can be used for various purposes, including:\nThe significance of variables lies in their ability to simplify complex tasks, enhance automation, and improve the overall efficiency of the Unix/Linux environment.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#unixlinux-variables",
    "href": "vars.html#unixlinux-variables",
    "title": "Variables",
    "section": "",
    "text": "Customizing the user environment (e.g., setting the default editor).\n\nStoring temporary data within scripts.\n\nPassing data between programs and scripts.\n\nConfiguring system and application settings.\n\n\n\nEnvironment vs. Shell\nIn Unix/Linux, variables are broadly categorized into environment variables and shell variables.\nEnvironment variables are global variables that are available system-wide and can influence the behavior of the shell and other programs. They are inherited by all child processes.\nEnvironment variables are often used to configure the shell environment, set application settings, and pass data to child processes.\n\nCommon environment variables\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPATH\nSpecifies the directories to search for executable files.\n\n\nHOME\nRepresents the current user’s home directory.\n\n\nUSER\nContains the name of the current user.\n\n\nLANG\nDefines the language and locale settings for applications.\n\n\n\nShell variables are local to the shell session in which they are defined. They are not inherited by child processes unless explicitly exported.\nShell variables are typically used for temporary data storage within scripts, customizing the shell prompt, and managing session-specific settings.\n\nCommon shell variables\n\n\nVariable\nDescription\n\n\n\n\nPS1\nDefines the primary prompt string.\n\n\nPS2\nDefines the secondary prompt string.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#setting-variables",
    "href": "vars.html#setting-variables",
    "title": "Variables",
    "section": "Setting Variables",
    "text": "Setting Variables\nIn the following sections we’ll cover setting shell and environment variables in your shell. These commands will work in Bash or Zsh.\n\nSetting Shell Variables\nShell variables in Unix/Linux are defined within the context of the current shell session. To set a shell variable, simply assign a value to a variable name using the following syntax:\nVARIABLE_NAME=value\nVariable names are case-sensitive and traditionally use uppercase letters. When creating a new variable, there should be no spaces around the = sign. We can check the value of the new variable using echo:\n\nMY_VAR=\"Hello, World\"\n\n\necho $MY_VAR\n# Hello, World\n\n\n\n\nZsh  on macOS .\n\n\n\nRemoving Shell Variables\nTo remove a shell variable, use the unset command followed by the variable name:\n\nunset MY_VAR\n\nAfter running unset, the variable MY_VAR will no longer exist in the current shell session.\nTo list all shell variables, use the set command:\nset\n\n\nSetting Environment Variables\nEnvironment variables are set similarly to shell variables but are made available to child processes by using the export command. To set an environment variable, follow these steps:\n\nAssign a value to a variable.\nExport the variable.\n\n\nMY_ENV_VAR=\"This is an environment variable\"\nexport MY_ENV_VAR\n\nYou can also combine these steps into a single command:\n\nexport MY_ENV_VAR=\"This is an environment variable\"\n\nTo verify the value of an environment variable, use the echo command:\n\n\n\nZsh  on macOS .\n\n\necho $MY_ENV_VAR\n# This is an environment variable\n\nTo list all environment variables, use the printenv or env command:\n\nprintenv\n\nor\n\nenv",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#using-variables",
    "href": "vars.html#using-variables",
    "title": "Variables",
    "section": "Using Variables",
    "text": "Using Variables\nThe section below covers how to use environment and shell variables in the terminal and in shell script (.sh) files.\n\nUsing Shell Variables\nUsing shell variables involves referencing them within scripts or commands with a dollar sign ($) prefix and the variable. Below is an example of using a shell variable in an if/then loop:\n\ncount=5\nif [ $count -gt 3 ]; then \n  echo \"Count is greater than 3\"; \nfi\n# Count is greater than 3\n\nThe code above could be stored in a script (if_count.sh), where the shell variables could be used to store and manipulate data dynamically by calling source:\n\ntouch if_count.sh\necho \"count=5\nif [ $count -gt 3 ]; then \n  echo \"Count is greater than 3\"; \nfi\" &gt; if_count.sh\n\n\nsource if_count.sh\n# if_count.sh: line 2: [: -gt: unary operator expected\n\n\n\nUsing Environment Variables\nEnvironment variables can be accessed in the same way as shell variables by prefixing the variable name with a dollar sign ($). Since they are available to child processes, environment variables are particularly useful for configuring application settings and passing data to scripts and programs.\nFor example, PWD works similar to the pwd command, but the path is stored as a string to use in scripts or programs.\n\necho $PWD\n# /Users/username/projects/books/fm-unix\n\nIn a script, we can use environment variables to ensure that certain settings or configurations are applied (like HOME and USER)\nAssume the following is stored in my_config.sh\n#!/bin/bash\necho \"User's home directory: $HOME\"\necho \"Current user: $USER\"\necho \"Current language: $LANG\"\nThese variables don’t need to be defined in the script–they can just be stored and called directly:\n\nsource my_config.sh\n# User's home directory: /Users/username\n# Current user: username\n# Current language: en_US.UTF-8",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#key-differences",
    "href": "vars.html#key-differences",
    "title": "Variables",
    "section": "Key Differences",
    "text": "Key Differences\nUnderstanding the difference between environment and shell variables is essential for effective Unix/Linux system administration and scripting. It helps users manage and customize their environment efficiently, ensuring that scripts and applications work as intended.\n\nScope\n\n\n\n\n\n\nEnvironment variables have a global scope and are available to all processes.\n\n\nShell variables have a local scope and are limited to the shell session.\n\n\n\n\n\nInheritance\n\n\n\n\n\n\nEnvironment variables are inherited by child processes.\n\n\nShell variables need to be exported to be available to child processes.\n\n\n\n\n\nPurpose\n\n\n\n\n\n\nEnvironment variables are generally used for system-wide configuration and application settings.\n\n\nShell variables are used for script-specific or session-specific tasks.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#recap",
    "href": "vars.html#recap",
    "title": "Variables",
    "section": "Recap",
    "text": "Recap\nBy understanding and utilizing shell and environment variables effectively, you can enhance the functionality and flexibility of your Unix/Linux scripts and system configurations. We’ll touch on this topic again in the shell scripts section.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "Text",
    "section": "",
    "text": "Plain Text Files\nThe essence of Unix and Linux systems is their powerful simplicity, and nowhere is this more evident than in how they handle text. Text commands and editors form the backbone of these operating systems, enabling users to perform complex tasks with a few keystrokes. This chapter will delve into pattern matching with regular expressions, the fundamental concepts of plain text files, text streams, and the Unix philosophy that prioritizes text as a universal interface for programming and system administration.\nPlain text files are vital to Unix/Linux systems, embodying their philosophy of ‘simple and beautiful.’1 These files only contain text, making them versatile and powerful. Unix/Linux believes that ‘everything is a file,’ including devices, configurations, and processes.2 Plain text files are the universal interface between systems, programs, and users. Standard Unix/Linux tools can easily create, manipulate, and read plain text, making it an essential interface for system administration, programming, and process management.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#plain-text-files",
    "href": "text.html#plain-text-files",
    "title": "Text",
    "section": "",
    "text": "Simple and Efficient\nPlain text files are simple, versatile, and easy to work with. They can be edited with any text editor and don’t require specialized software, making actions transparent and learning accelerated. Plain text can also be easily manipulated using standard Unix/Linux text-processing tools such as grep, sed, and awk. With simple one-liners from the command line, users can search for a specific line, replace text across multiple files, or transform data formats.\n\n\nCommunication Between Programs\nUnix/Linux philosophy values specialized programs that work together efficiently. Plain text files are used as inputs or outputs in pipelines of simple, single-purpose programs to perform complex operations.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-streams",
    "href": "text.html#text-streams",
    "title": "Text",
    "section": "Text Streams",
    "text": "Text Streams\nA text stream in Unix and Linux is a simple, sequential flow of characters. Text streams can be inputs from keyboards, outputs to a display screen, or the data within a file. The concept of text streams is fundamental to the Unix philosophy; it allows for the chaining together of commands, where the output of one command can be seamlessly passed as input to another through a mechanism known as piping.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-editors-and-the-unix-philosophy",
    "href": "text.html#text-editors-and-the-unix-philosophy",
    "title": "Text",
    "section": "Text Editors and The Unix Philosophy",
    "text": "Text Editors and The Unix Philosophy\nThe Unix philosophy emphasizes simplicity, clarity, and the principle of “doing one thing well.” Plain text embodies this philosophy, serving as a simple, straightforward, and versatile means of interaction between the user, the system, and the programs running on it. This philosophy also underpins the design of Unix text editors, which range from the simple (like nano) to the powerful and extensible (like vi and emacs).\nText commands and editors are not just tools but the medium through which users communicate with the system and manipulate it to their will. Mastering these commands and editors opens up a world of possibilities for efficient system management, programming, and beyond.\nThis section will explore the core text commands that every Unix and Linux user should know, from file manipulation to text processing and searching. We will also introduce the most popular text editors, guiding you through their primary usage and highlighting their unique features.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#footnotes",
    "href": "text.html#footnotes",
    "title": "Text",
    "section": "",
    "text": "Doug McIlroy on Unix programming: “Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.” - Wikipedia↩︎\nLinus Torvalds (creator and lead developer of the Linux kernel) has clarified this to, “The UNIX philosophy is often quoted as ”everything is a file”, but that really means ”everything is a stream of bytes.”↩︎",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "symbols_patterns.html",
    "href": "symbols_patterns.html",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Wildcards\nRegular expressions are powerful tools for searching and manipulating text data. A regular expression (or ‘regex’) is made up of special symbols that define specific patterns to be identified or transformed. The regex patterns are not the data themselves, but rather a framework for locating or modifying text data. In this chapter, we’ll explore wildcards, regular expressions, and other special characters.\nWildcards (also known as glob patterns) are mostly used in commands to match filenames, paths, or filter text (ls, cp, mv, rm, etc.). Arguments can include wildcards, which the shell expands into a list of files or directories that match the pattern.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-wildcards",
    "href": "symbols_patterns.html#sec-wildcards",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Asterisk: *\n* is a wildcard for matching zero or more characters.\nExample\nls *.md lists all files in the data/ directory that end with .md:\n\nls data/*.md\n# data/README.md\n\n\n\nQuestion Mark: ?\n? is the wildcard for matching exactly one character.\nExample\nls myfile?.txt lists files like myfile2.txt, but not myfile.txt and my file 3.txt:\n\nls myfile?.txt\n# myfile2.txt\n\n\n\nSquare brackets: []\n[abc]: Matches any one character listed (a, b, or c).\nExample\n[a-z]: Matches any one character (n, e, or w).\n\nls [new]*.txt\n# newfile.txt\n\nExample\nNatch any one character in range (a to p).\n\nls data/[a-p]*\n# data/ajperlis_epigrams.txt\n# data/music_vids.tsv\n# data/pwrds.csv\n# data/pwrds.tsv",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-regexp",
    "href": "symbols_patterns.html#sec-regexp",
    "title": "Symbols & Patterns",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions (or the singular ‘regex’) are powerful tools for searching and manipulating text data. A regex is made up of special symbols that define specific patterns to be identified or transformed.\nRegular expressions operate on text–the sequence of characters that can include letters, digits, punctuation, and other character types. Text serves as the ‘data’ or ‘medium’ for which the patterns the regex describes are searched.\nRegular expressions are more complex than wildcards, and are typically used with tools like grep (global regular expression print), sed (stream editor), and awk.\n\nDot: .\n. matches any single character except a newline.\nExamples\nMatches lines containing “password” or similar patterns where any character stands between ‘p’ and ‘ssword’.\n\ngrep \"p.ssword\" data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n\nReplaces “password” where any character is between ‘p’ and ‘ssword’ with “p@ssword”.\n\nsed 's/p.ssword/p@ssword/' data/pwrds.csv | head -n2\n# p@ssword,rank,strength,online_crack\n# p@ssword,1,8,6.91 years\n\nSelect records where “password” or similar patterns appear with any character between ‘p’, ‘ssw and rd’.\n\nawk '/p.ssw.rd/' data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n# passw0rd,500,28,92.27 years\n\n\n\nAsterisk: *\n* matches zero or more of the preceding element.\nExamples\nWe can use grep to find lines where “i” is followed by zero or more “l”s (including none):\n\ngrep 'il*' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Ghostface Killah  Dennis Coles\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nWe can use sed to replace two or more \"l\"s with 11:\n\nsed 's/lll*/11/g' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Ki11ah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Ki11a   Jamel Irief\n# Cappadonna    Darryl Hi11\n# Ol Dirty Bastard  Russe11 Tyrone Jones\n\nPrint lines that start with one or more \"R\"s\n\nawk '/^ *R/' data/wu_tang.txt\n# RZA   Robert Diggs\n# Raekwon the Chef  Corey Woods\n\n\n\nPlus: +\n+ matches one or more occurrences of the preceding element.\nExamples\nUse grep with extended regular expressions to find ‘i’ followed by one or more ’l’s:\n\ngrep -E 'il+' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n\nReplace one or more \"a\"s with the @:\n\nsed -E 's/a+/@/g' data/wu_tang.txt\n# Member    N@me\n# RZA   Robert Diggs\n# GZA   G@ry Grice\n# Method M@n    Clifford Smith\n# R@ekwon the Chef  Corey Woods\n# Ghostf@ce Kill@h  Dennis Coles\n# Inspect@h Deck    J@son Hunter\n# U-God     L@mont H@wkins\n# M@st@ Kill@   J@mel Irief\n# C@pp@donn@    D@rryl Hill\n# Ol Dirty B@st@rd  Russell Tyrone Jones\n\nThe + operator needs the -E option to enable extended regular expressions.\nPrint lines with text containing one or more \"Z\"s:\n\nawk '/Z+/' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n\n\n\nQuestion Mark: ?\n? makes the preceding element optional (matches zero or one occurrence).\nExamples\nUse grep with extended regular expressions to find lines with ‘Killah’ or ‘Killah’:\n\ngrep -E 'Kill?' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n\nsed: Replace Ghostface with Ghost Face:\n\nsed -E 's/Ghostface?/Ghost Face/' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghost Face Killah Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nawk: Print lines with one or more digits.\n\nawk '/[0-9]+/' data/wu_tang.txt\n\n\n\nCharacter Set: [abc]\n[abc] matches any single character listed in the set.\nExample\nUse grep to find lines containing ‘a’, ‘b’, or ‘c’:\ngrep '[abc]' filename.txt\n\n\nCaret: ^\n^ matches the start of a line.\nExample\nUse grep to find lines that start with ‘start’:\ngrep '^start' filename.txt\n\n\nDollar: $\n$ matches the end of a line.\nExample\nUse grep to find lines that end with ‘end’:\ngrep 'end$' filename.txt\nThese patterns are extremely powerful in scripting and command-line operations for filtering and manipulating text data efficiently. Here’s how you might use them in combination across different tools:\n\nsed for substitution: Replace ‘foo’ with ‘bar’ only if ‘foo’ appears at the beginning of a line:\n\nsed 's/^foo/bar/' filename.txt\n\nawk for selection: Print lines where the first field matches ‘start’:\n\nawk '/^start/ {print $0}' filename.txt\n\nperl for advanced manipulation: Increment numbers found at the end of each line:\n\nperl -pe 's/(\\d+)$/ $1+1 /e' filename.txt",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#special-characters",
    "href": "symbols_patterns.html#special-characters",
    "title": "Symbols & Patterns",
    "section": "Special Characters",
    "text": "Special Characters\nSpecial Characters: Characters such as spaces, quotes, and others have special meanings in the shell. They need to be treated carefully when used within arguments.\n\nBraces: {}\nBrace Expansion: Similar to wildcards, brace expansion ({}) allows the creation of multiple text strings from a pattern containing braces.\nExample\ncat wu_tang.{txt,csv}\n\ncat data/wu_tang.{tsv,dat}\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nExpands into:\n\ncat data/wu_tang.tsv \ncat data/wu_tang.dat\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\n\n\nBackslash: \\\n\\ escapes the following character, nullifying its special meaning\nExample\necho \"File name with spaces \\& special characters\" prints the text with spaces and the ampersand:\n\necho \"File name with spaces \\& special characters\"\n# File name with spaces & special characters\n\n\n\nSingle quotes: ''\nSingle quotes (' ') treat every character literally, ignoring the special meaning of all characters.\nExample\necho '$HOME' prints $HOME, not the path to the home directory:\n\necho '$HOME'\n# $HOME\n\n\n\nDouble quotes: \"\"\nDouble quotes (\" \") allow for the inclusion of special characters in an argument, except for the dollar sign ($), backticks (` `), and backslash (\\).\nExample\necho \"$HOME\" prints the path to the home directory:\n\necho \"$HOME\"\n#&gt; /Users/username\n\n\n\nTilde: ~\n~ represents the home directory of the current user.\nExample\nList the items in the user’s home directory:\n\nls ~\n#&gt; Applications\n#&gt; Creative Cloud Files\n#&gt; Desktop\n#&gt; Documents\n#&gt; Downloads\n#&gt; Dropbox\n#&gt; Fonts\n#&gt; Library\n#&gt; Movies\n#&gt; Music\n#&gt; Pictures\n#&gt; Public\n#&gt; R\n#&gt; Themes\n\n\n\nDollar Sign: $\n$ indicates a variable.\nExample\necho $PATH prints the value of the PATH environment variable:\n\necho $PATH\n\n\n\nAmpersand: &\n& runs a command in the background.\nExample\nfirefox & opens Firefox in the background, allowing the terminal to be used for other commands.\n\nfirefox &\n\n\n\nSemicolon: ;\n; separates multiple commands to be run in sequence.\nExample\ncd data; ls changes the directory to data and then lists its contents:\n\ncd data; ls\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n\n\nGreater Than: &gt;\nRedirection operators: &gt; directs output to a file or a device.\nExample\necho \"This is my 2nd file\" &gt; myfile2.txt writes \"This is my 2nd file\" into myfile2.txt:\n\necho \"This is my 2nd file\" &gt; myfile2.txt\n\n\n\nLess Than: &lt;\nRedirection operators: &lt; takes input from a file or a device.\nExample\nThen wc &lt; myfile2.txt counts the words in myfile2.txt:\n\nwc &lt; myfile2.txt\n#        1       5      20\n\n\n\nParentheses: ()\nParentheses can be used to group commands or for command substitution with $( ).\nExample\n(cd /data; ls) runs ls in /data without changing the current directory:\n\n(cd data; ls)\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n$(command) uses the output of command.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "text_commands.html",
    "href": "text_commands.html",
    "title": "Manipulating Text",
    "section": "",
    "text": "The Text Stream\nThe Unix and Linux operating systems thrive on simplicity and efficiency, principles elegantly manifested in text handling. Central to this ecosystem are the commands explicitly designed for text manipulation.\nUnix/Linux conceptualizes text as a stream, a continuous sequence of characters that can be manipulated in real-time. Streams are crucial for understanding how Unix/Linux commands process text. A text stream can originate from files, input devices, or even the output of other commands. Treating text as a steady stream of inputs offers a versatile and powerful method for text manipulation.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#the-text-stream",
    "href": "text_commands.html#the-text-stream",
    "title": "Manipulating Text",
    "section": "",
    "text": "Refresher: Standard Input and Standard Output\n\n\n\n\n\n\nTwo key concepts in Unix text processing are standard input (stdin) and standard output (stdout). stdin is the default input stream, which often comes from the keyboard or the output of another command. stdout is the default output stream, typically the terminal screen. Many Unix commands read from stdin when no file is specified and write to stdout, allowing the output of one command to become the input of another. This design facilitates the chaining of commands (piping) to perform complex operations in a streamlined manner.\n\nInput generally refers to the data fed into a command, which can come from stdin or be specified as arguments.\nOutput is the data produced by a command, displayed on stdout unless redirected.\n\nThis interconnectivity of stdin and stdout, all communicating through text streams, exemplifies the efficiency and flexibility of Unix-like systems.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#text-manipulation",
    "href": "text_commands.html#text-manipulation",
    "title": "Manipulating Text",
    "section": "Text Manipulation",
    "text": "Text Manipulation\nText manipulation commands embody the Unix philosophy of ‘do one thing and do it well’ and demonstrate the system’s power in processing text streams. This section will explore these fundamental commands, illustrating how they easily interact with text streams, standard input (stdin), and standard output (stdout) to perform complex text manipulations.\n\nWu-Tang\nThe data/ folder contains three different file formats of the members of the Wu-Tang American hip hop collective. We’ll use the tree command to view the contents of the data/ directory:1\n\ntree -P 'wu*' data\n\nThe -P 'wu*' option tells tree to include only files and directories that match the pattern 'wu*'. The pattern here uses a wildcard (*), meaning it will match any file or directory name that starts with \"wu\". The pattern is case-sensitive by default.\n\n# data\n# ├── wu_tang.csv\n# ├── wu_tang.dat\n# ├── wu_tang.tsv\n# └── wu_tang.txt\n# \n# 1 directory, 4 files\n\nWe can add the -f option to instruct tree to display the full path for each file and directory relative to the root of the tree, instead of just showing the names.\n\ntree -Pf 'wu*' data\n\nThe command summarizes the content by showing “1 directory, 4 files”.2\n\n# data\n# ├── data/wu_tang.csv\n# ├── data/wu_tang.dat\n# ├── data/wu_tang.tsv\n# └── data/wu_tang.txt\n# \n# 1 directory, 4 files\n\nThe wu_tang.dat file contains the members in pipe-delimited format:\n\ncat data/wu_tang.dat\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nWe can use head and tail to view specific ‘rows’ of the data:\n\nhead -n8 data/wu_tang.dat | tail -n4\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n\n\n\nEpigrams\necho prints its arguments to the standard output (stdout). It can be used in scripts and on the command line to display messages or variables.\n\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\"\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\necho can also be used to write text to a file created with touch. The quote below comes from Alan Perlis’s 1982 article, “Epigrams on Programming.”3\n\ntouch data/turing_tarpit.txt\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\" &gt; data/turing_tarpit.txt\n\ncat displays the content of files straight to the screen, useful for checking what’s in a file quickly.\n\ncat data/turing_tarpit.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\nAll 130 epigrams are stored and numbered in the data/perlis_epigrams.txt file.\nhead and tail allow us to view the top and bottom of any text file:\n\nhead data/ajperlis_epigrams.txt\n# One man’s constant is another man’s variable.\n# Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# Syntactic sugar causes cancer of the semicolon.\n# Every program is a part of some other program and rarely fits.\n# If a program manipulates a large amount of data, it does so in a small number of ways.\n# Symmetry is a complexity-reducing concept (co-routines include subroutines); seek it everywhere.\n# It is easier to write an incorrect program than understand a correct one.\n# A programming language is low level when its programs require attention to the irrelevant.\n# It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.\n# Get into a rut early: Do the same process the same way. Accumulate idioms. Standardize. The only difference(!) between Shakespeare and you was the size of his idiom list - not the size of his vocabulary.\n\n\ntail data/ajperlis_epigrams.txt\n# In seeking the unattainable, simplicity only gets in the way. If there are epigrams, there must be meta-epigrams.\n# Epigrams are interfaces across which appreciation and insight flow.\n# Epigrams parametrize auras.\n# Epigrams are macros, since they are executed at read time.\n# Epigrams crystallize incongruities.\n# Epigrams retrieve deep semantics from a data base that is all procedure.\n# Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# Epigrams are more like vitamins than protein.\n# Epigrams have extremely low entropy.\n# The last epigram? Neither eat nor drink them, snuff epigrams.\n\nApplying a ‘trust, but verify’ to the previous claim about the Turing tar-pit quote involves using grep (“global regular expression print”) to confirm the text in turing_tarpit.txt is also in dataperlis_epigrams.txt.\ngrep reads from stdin (or a list of files) and outputs the lines that match a specified pattern. Lets see how many epigrams in data/perlis_epigrams.txt include the word “Turing”:\n\ngrep Turing data/ajperlis_epigrams.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\nNumber sequences\nWe’ll add numbers to each of the 130 epigrams in data/ajperlis_epigrams.txt to make them easier to reference. We can use the seq command piped into a formatting command like awk:\n\nseq 130 | awk '{print $1\".\"}' | head\n# 1.\n# 2.\n# 3.\n# 4.\n# 5.\n# 6.\n# 7.\n# 8.\n# 9.\n# 10.\n\nseq 130 generates the sequence of numbers from 1 to 130, then awk '{print $1\") \"}' takes uses the numbers from seq as the input to awk ($1) and appends a period . to each number. We can add the &gt; operator redirects the output to data/numbered_lines.txt.\n\nseq 130 | awk '{print $1\".\"}' &gt; data/numbered_lines.txt\n\nWe can now use paste to combine data/numbered_lines.txt and data/ajperlis_epigrams.txt. The -d option stands is the delimiter to be placed between the pasted lines (which we’ll use to specify a space \" \").\nWe’ll preview the head and tail of our paste before writing to a file:\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | head -n5\n# 1. One man’s constant is another man’s variable.\n# 2. Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# 3. Syntactic sugar causes cancer of the semicolon.\n# 4. Every program is a part of some other program and rarely fits.\n# 5. If a program manipulates a large amount of data, it does so in a small number of ways.\n\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | tail -n5\n# 126. Epigrams retrieve deep semantics from a data base that is all procedure.\n# 127. Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# 128. Epigrams are more like vitamins than protein.\n# 129. Epigrams have extremely low entropy.\n# 130. The last epigram? Neither eat nor drink them, snuff epigrams.\n\nAll 130 epigrams line up, so we’ll assign the output to the data/numbered_epigrams.txt file.\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt &gt; \\\n  data/numbered_epigrams.txt\n\nNow we can re-check our Turing pattern in data/numbered_epigrams.txt:\n\ngrep Turing data/numbered_epigrams.txt\n# 54. Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# 83. What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\n\n\nRoxanne\nThe data/roxanne.txt file contains the lyrics to the 1979 song Roxanne by The Police. We’ll use this file to explore several powerful Unix/Linux command-line utilities that are invaluable for searching, editing, and manipulating text data in files.\n\nGlobal substitutions\nawk is a powerful text processing tool. Here’s an example where awk uses gsub (global substitution) to replace the phrase “red light” with “green light” in the lyrics:\n\nawk '{gsub(/red light/, \"green light\"); print}' \\\n  data/roxanne.txt | head -4\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't have to sell your body to the night\n\ngsub(/red light/, \"green light\") tells awk to substitute \"red light\" with \"green light\" globally within each line. print outputs the modified line. Without this, awk would not display anything. The entire command is enclosed in single quotes to prevent the shell from interpreting any special characters.\nIf we wanted to replace \"Roxanne\" with \"Dianne\" throughout the song, we’d use:\n\nsed 's/Roxanne/Dianne/g' data/roxanne.txt\n\n\n\nshow/hide output\n# Dianne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n# Dianne\n# You don't have to wear that dress tonight\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Ro...\n# I loved you since I knew you\n# I wouldn't talk down to you\n# I have to tell you just how I feel\n# I won't share you with another boy\n# I know my mind is made up\n# So put away your make up\n# Told you once I won't tell you again\n# It's a bad way\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n\n\nThe s stands for substitute, the pattern to be replaced (\"Roxanne\") is followed by the new text (\"Dianne\"), and the g at the end of the command tells sed to perform the substitution globally on each line, rather than stopping after the first occurrence.\nsort arranges lines of text alphabetically or numerically and uniq filters out adjacent repeated lines in a file (often used in conjunction with sort).\n\nsort data/roxanne.txt | uniq\n\n\n\nshow/hide output\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\n\nThe commands above sort the lines in the file first, then filter repeated lines.\nWe’ll use awk to add line numbers to data/roxanne.txt. NR is the record number variable in awk, which counts the lines. $0 represents the entire current line, and combining them with print will print the line number followed by the original line.\n\nawk '{print NR, $0}' data/roxanne.txt &gt; data/roxanne_lined.txt\nhead -n5 data/roxanne_lined.txt \n# 1 Roxanne\n# 2 You don't have to put on the red light\n# 3 Those days are over\n# 4 You don't have to sell your body to the night\n# 5 Roxanne\n\n\n\nTwo-file commands\nWe’ll create a ‘metadata’ file for Roxanne in data/roxanne_meta.txt and add some content:\n\ntouch data/roxanne_meta.txt\necho \"1 &lt;Song Title&gt;\n2 &lt;Chorus&gt;\n3 &lt;Verse 1&gt;\n4 &lt;Verse 2&gt;\n5 &lt;Song Title&gt;\" &gt; data/roxanne_meta.txt\ncat data/roxanne_meta.txt\n# 1 &lt;Song Title&gt;\n# 2 &lt;Chorus&gt;\n# 3 &lt;Verse 1&gt;\n# 4 &lt;Verse 2&gt;\n# 5 &lt;Song Title&gt;\n\njoin is used to combine two files based on a common field. Assuming there’s another file with additional details for some lines, you would use:\n\njoin -1 1 -2 1 data/roxanne_lined.txt data/roxanne_meta.txt\n# 1 Roxanne &lt;Song Title&gt;\n# 2 You don't have to put on the red light &lt;Chorus&gt;\n# 3 Those days are over &lt;Verse 1&gt;\n# 4 You don't have to sell your body to the night &lt;Verse 2&gt;\n# 5 Roxanne &lt;Song Title&gt;\n\n-1 1 specifies that the join field for the first file (data/roxanne_lined.txt) is the first column, and -2 1 means that the join field for the second file (data/roxanne_meta.txt) is also the first column.\ncomm is used to compare two sorted files line by line and outputs three columns by default:\n1. Lines unique to the first file. 2. Lines unique to the second file. 3. Lines common to both files.\nLet’s assume we have two versions of the song “Roxanne”. The original version is stored in roxanne_orig.txt, and a revised version with some lines changed, added, or removed is stored in roxanne_rev.txt.\nroxanne_orig.txt\n\ntouch data/roxanne_orig.txt\necho \"Roxanne\nYou don't have to put on the red light\nThose days are over\nYou don't have to sell your body to the night\" &gt; data/roxanne_orig.txt\n\nroxanne_rev.txt\n\ntouch data/roxanne_rev.txt\necho \"Roxanne\nYou don't have to put on the green light\nThose days are over\nYou don't need to sell your dreams to the night\" &gt; data/roxanne_rev.txt\n\nThese files are structured to have similar content with minor differences.\n\n\n\n\n\n\n# Roxanne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n\n\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't need to sell your dreams to the night\n\n\n\nFirst, ensure both files are sorted (if not already). For simplicity, let’s assume these are sorted or have matching line orders. Then, use comm:\n\ncomm data/roxanne_orig.txt data/roxanne_rev.txt\n#       Roxanne\n#   You don't have to put on the green light\n#   Those days are over\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n#   You don't need to sell your dreams to the night\n\nOutput:\n\nThe first column shows lines that are only in the original file (roxanne_orig.txt).\n\n\n\n\n\n\n\nThe second column shows lines that are only in the revised file (roxanne_rev.txt).\n\n\n\n\n\n\n\nThe third column shows lines that are common in both files.\n\n\n\n\n\n\nWe can suppress any of these columns using the -1, -2, or -3 options. For example, comm -12 data/roxanne_orig.txt data/roxanne_rev.txt will show only the lines that are common in both files:\n\ncomm -12 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nOr\n\ncomm -1 -2 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nNOTE: ensure the files are sorted on the lines you are comparing; otherwise, comm will not function correctly.\nTo see the line by line differences between data/roxanne_orig.txt and data/roxanne_rev.txt, pass both files to diff:\n\ndiff data/roxanne_orig.txt data/roxanne_rev.txt\n# 2c2\n# &lt; You don't have to put on the red light\n# ---\n# &gt; You don't have to put on the green light\n# 4c4\n# &lt; You don't have to sell your body to the night\n# ---\n# &gt; You don't need to sell your dreams to the night\n\nThe output of differences from diff can be interpreted as follow:\n\n2c2 indicates that a change has been made at line 2 of both files. The c stands for “change”.\n\n&lt; You don't have to put on the red light shows what line 2 looked like in the original file (roxanne_orig.txt)\n--- is a separator used by diff to distinguish between the old version and the new version of the line\n&gt; You don't have to put on the green light: shows what line 2 now looks like in the revised file (roxanne_rev.txt)\n\n4c4 indicates a change at line 4 in both documents.\n\n&lt; You don't have to sell your body to the night shows the original text at line 4 in roxanne_orig.txt\n---: Again, a separator\n&gt; You don't need to sell your dreams to the night shows the revised text at line 4 in roxanne_rev.txt\n\n\nThe -y option will display the changes side-by-side\n\ndiff -y data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne                                                                 Roxanne\n# You don't have to put on the red light                          |       You don't have to put on the green light\n# Those days are over                                                     Those days are over\n# You don't have to sell your body to the night                   |       You don't need to sell your dreams to the night\n\nOr, if all we care about is if the files differ, we can use the -q option:\n\ndiff -q data/roxanne_orig.txt data/roxanne_rev.txt\n# Files data/roxanne_orig.txt and data/roxanne_rev.txt differ",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#recap",
    "href": "text_commands.html#recap",
    "title": "Manipulating Text",
    "section": "Recap",
    "text": "Recap\nUsing these commands can dramatically enhance productivity and efficiency when working with text files in Unix/Linux environments.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#footnotes",
    "href": "text_commands.html#footnotes",
    "title": "Manipulating Text",
    "section": "",
    "text": "The tree command is not available in every Shell, but you can install it using Homebrew.↩︎\nNote on Directory and File Structure: The output structure from tree -Pf 'wu*' data visually shows that the data directory contains only the files matching the pattern and no subdirectories under it that match the pattern (or any subdirectories at all in this context). The -P option does not cause tree to exclude other directories from the inspection; it only filters what is displayed based on the pattern. If there were non-matching files or subdirectories, they would not appear in the output due to the filter.↩︎\nRead the Wikipedia or download the original PDF.↩︎",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_editors.html",
    "href": "text_editors.html",
    "title": "Text Editors",
    "section": "",
    "text": "nano",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#vi-vim",
    "href": "text_editors.html#vi-vim",
    "title": "Text Editors",
    "section": "vi (Vim)",
    "text": "vi (Vim)\nVim, short for Vi IMproved, is an advanced text editor that is an enhanced version of the vi editor common to UNIX systems. Vim designed for both casual text editing and complex code development, making it a popular choice for developers and system administrators alike. Vim is known for its power, flexibility, and efficiency, but it can be intimidating for beginners due to its modal nature and extensive commands.\n\nGetting Started\nTo start using Vim, you can simply type vim followed by the name of the file you wish to edit or create. For example:\n\nvim example.txt\n\nThis command will open example.txt in Vim. If the file doesn’t exist, Vim will create it once you attempt to save.\n\n\nVim Modes\nVim operates in several modes, primarily:\n\nNormal Mode: The default mode where you can use vim commands. No text insertion happens in this mode.\nInsert Mode: Allows you to insert text. Enter this mode by pressing i in Normal Mode.\nCommand Mode: Accessed from Normal Mode by pressing :. Here, you can execute Vim commands and scripts.\n\n\n\nBasic Commands\nHere are some fundamental commands to get you started:\n\ni - Enter insert mode to start typing/editing the text.\nEsc - Return to normal mode from any other mode.\n:w - Save the changes made to the file.\n:q - Quit Vim.\n:wq - Save the changes and quit Vim.\ndd - Delete (cut) a line in normal mode.\nyy - Copy (yank) a line in normal mode.\np - Paste the copied or deleted line below the current line.\n\n\n\nExample Session\nLet’s consider a simple session where you edit a new file:\n\nOpen or create a file:\nvim example.txt\nEnter Insert Mode to start typing:\ni\nType your text, for instance:\nHello, this is a test file with Vim.\nPress Esc to go back to Normal Mode.\nSave and exit:\n:wq",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#advanced-features",
    "href": "text_editors.html#advanced-features",
    "title": "Text Editors",
    "section": "Advanced Features",
    "text": "Advanced Features\nAs you grow more comfortable with the basics of Vim, you may explore its advanced features:\n\nMultiple Windows: Open multiple files or views using :split, :vsplit\nMacros: Automate repetitive tasks by recording them.\nCustomizable Settings: Tweak Vim’s behavior through a .vimrc file\nPlugins: Extend Vim’s functionality with plugins like NERDTree for file system navigation or YouCompleteMe for code completion.\n\nWhile Vim has a steep learning curve, mastering it can significantly enhance your productivity and efficiency in text editing tasks. Start with basic commands, gradually exploring more complex features as you become more comfortable with the editor.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#emacs",
    "href": "text_editors.html#emacs",
    "title": "Text Editors",
    "section": "emacs",
    "text": "emacs\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Shell Scripts",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts"
    ]
  },
  {
    "objectID": "format.html",
    "href": "format.html",
    "title": "Format",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n# Create and populate the file with your data\ntouch who_tb_data.txt\necho \"country year  type  count\nAfghanistan 1999  cases 745\nAfghanistan 1999  population  19987071\nAfghanistan 2000  cases 2666\nAfghanistan 2000  population  20595360\nBrazil  1999  cases 37737\nBrazil  1999  population  172006362\nBrazil  2000  cases 80488\nBrazil  2000  population  174504898\nChina 1999  cases 212258\nChina 1999  population  1272915272\nChina 2000  cases 213766\nChina 2000  population  1280428583\" &gt; who_tb_data.txt\n\n# Get the word count values\ncounts=$(wc who_tb_data.txt | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\nTo make the commands above more generalizable so that any file can be passed as input rather than being restricted to a specific file (who_tb_data.txt), we can modify the script to take a filename as a command-line argument.\nThis way, you can use the script with any file by specifying the filename when you run the script.\n\nStep 1: Modify the Script to Take Command-Line Arguments\nHere’s how the revised script could look:\n#!/bin/bash\n\n# Check if a file name was provided as an argument\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 &lt;filename&gt;\"\n    exit 1\nfi\n\n# Check if the file exists\nif [ ! -f \"$1\" ]; then\n    echo \"File not found: $1\"\n    exit 1\nfi\n\n# Get the word count values\ncounts=$(wc \"$1\" | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\n\nStep 2: Save and Make the Script Executable\n\nSave the script in a file, for example, format_wc_output.sh.\nMake sure the script is executable:\nchmod +x format_wc_output.sh\n\n\n\nStep 3: Run the Script with a File as an Argument\n\nNow you can run the script with any file as an argument. For example:\n./format_wc_output.sh somefile.txt\n\n\n\nExplanation\n\nArgument Checking: The script now starts by checking if exactly one argument (the filename) is provided. If not, it prints a usage message and exits. This ensures the user knows how to run the script correctly.\nFile Existence Checking: It checks if the file exists before attempting to process it. If the file doesn’t exist, it prints an error message and exits. This prevents errors related to non-existent files.\nUsing Command-Line Argument: The wc command now uses $1, which is a placeholder for the first command-line argument provided to the script (i.e., the filename you want to process).\n\nThis version of the script is more flexible and useful, as it can handle any file input, making it a handy tool for quickly formatting word count output for various files across your system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Format</span>"
    ]
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "Permissions",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nFile permissions with chmod\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Permissions</span>"
    ]
  },
  {
    "objectID": "part_two.html",
    "href": "part_two.html",
    "title": "Part 2: Use Cases",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Part 2: Use Cases]{style=\"font-size: 1.05em; font-weight: bold;\"}</span>"
    ]
  },
  {
    "objectID": "maintenance.html",
    "href": "maintenance.html",
    "title": "Maintenance",
    "section": "",
    "text": "Installations",
    "crumbs": [
      "Maintenance"
    ]
  },
  {
    "objectID": "maintenance.html#updates",
    "href": "maintenance.html#updates",
    "title": "Maintenance",
    "section": "Updates",
    "text": "Updates",
    "crumbs": [
      "Maintenance"
    ]
  },
  {
    "objectID": "maintenance.html#upgrades",
    "href": "maintenance.html#upgrades",
    "title": "Maintenance",
    "section": "Upgrades",
    "text": "Upgrades",
    "crumbs": [
      "Maintenance"
    ]
  },
  {
    "objectID": "installs.html",
    "href": "installs.html",
    "title": "Installations",
    "section": "",
    "text": "Package managers\nScenario: You’ve been asked to install a software package on your Linux operating system. This could be an applications, library, utility, or their dependencies.\nIn the Linux ecosystem, apt, yum, and dnf are package managers. These tools are responsible for installing and managing software packages, such as installing, updating, and removing software, as well as resolving package dependencies.",
    "crumbs": [
      "Maintenance",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installs.html#package-managers",
    "href": "installs.html#package-managers",
    "title": "Installations",
    "section": "",
    "text": "apt\napt (Advanced Package Tool)is used on Debian-based distributions like Ubuntu, Debian, etc. apt installs packages from .deb files, which are precompiled software packages.\n\nExample usage\n\nInstall a package:\nsudo apt install &lt;package_name&gt;\nUpdate a package:\nsudo apt update\nUpgrade installed packages:\nsudo apt upgrade\n\n\n\n\nyum\nyum (Yellowdog Updater, Modified) is used on older Red Hat-based distributions like CentOS 7, RHEL 7, and Fedora. yum installs packages from .rpm files, which are precompiled software packages.\n\nExample usage\n\nInstall a package:\nsudo yum install &lt;package_name&gt;\nUpdate all packages:\nsudo yum update\nRemove a package:\nsudo yum remove &lt;package_name&gt;\n\n\n\n\ndnf\ndnf (Dandified YUM) is the next-generation version of yum and the default package manager for RHEL 8+, CentOS 8+, and Fedora.\n\nMore efficient and faster than yum, with better dependency management.\n\n\nExample usage\n\nInstall a package:\nsudo dnf install &lt;package_name&gt;\nUpdate all packages:\nsudo dnf update\nRemove a package:\nsudo dnf remove &lt;package_name&gt;",
    "crumbs": [
      "Maintenance",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installs.html#recap",
    "href": "installs.html#recap",
    "title": "Installations",
    "section": "Recap",
    "text": "Recap\nThe apt, yum, and dnf package managers install:\n\nSoftware packages: Precompiled binaries or scripts, such as applications, libraries, and utilities.\nDependencies: Additional packages required for the primary software to function correctly.\nUpdates: Security patches and newer versions of existing software.\n\nThese package managers fetch packages from their respective repositories, which are online or local directories containing approved and signed software packages.",
    "crumbs": [
      "Maintenance",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.",
    "crumbs": [
      "Performance"
    ]
  },
  {
    "objectID": "file_size.html",
    "href": "file_size.html",
    "title": "Finding Large Files",
    "section": "",
    "text": "find\nScenario: You’ve been asked to discover any files over a certain size. This could be due concerns over disk space, performance optimization, backup and recovery, network load reduction, compliance, auditing, a system migration, user quotas, or internal data management policies (usually related to cost).",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#du-sort",
    "href": "file_size.html#du-sort",
    "title": "Finding Large Files",
    "section": "du & sort",
    "text": "du & sort",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#awk-ls",
    "href": "file_size.html#awk-ls",
    "title": "Finding Large Files",
    "section": "awk & ls",
    "text": "awk & ls",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#ncdu",
    "href": "file_size.html#ncdu",
    "title": "Finding Large Files",
    "section": "ncdu",
    "text": "ncdu",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#grep-find-or-du",
    "href": "file_size.html#grep-find-or-du",
    "title": "Finding Large Files",
    "section": "grep & find or du",
    "text": "grep & find or du",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding Large Files</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html",
    "href": "syntax_ref.html",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "Help & Documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#help-documentation",
    "href": "syntax_ref.html#help-documentation",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "man\nInterface to the on-line reference manuals.\n\n\nhelp\nDisplay help for built-in commands.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#files-directories",
    "href": "syntax_ref.html#files-directories",
    "title": "Appendix A — Syntax Reference",
    "section": "Files & Directories",
    "text": "Files & Directories\n\ncd\nChange directory.\n\n\nls\nList directory contents.\n\n\npwd\nPrint working directory.\n\n\nmkdir\nCreate a directory.\n\n\ntree\nDisplay the directory structure of a path in a tree-like format, showing all the files and subdirectories under it.\n\n\ncp\nCopy files or directories.\n\n\nmv\nMove or rename files or directories.\n\n\nrm\nRemove files or directories. By default, rm won’t remove a directory without the -R or -r option.\n\n\n\nln\nCreate links between files.\n\n\nreadlink\nreadlink displays the target of a symbolic link.\n\n\ntouch\nCreate a new file or update the timestamp of existing files.\n\n\nfind\nSearch for files in a directory hierarchy.\n\n\nlocate\nFind files by name quickly using a database.\n\n\nfile\nDetermine file type.\n\n\nstat\nDisplay file or file system status.\n\n\nmore\nView file contents interactively.\n\n\n\nmore iTerm2\n\n\n\n\nless\nView file contents interactively.\n\n\n\nless iTerm2\n\n\n\n\nhead\nOutput the first part of files.\n\n\ntail\nOutput the last part of files.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#system-administration",
    "href": "syntax_ref.html#system-administration",
    "title": "Appendix A — Syntax Reference",
    "section": "System Administration",
    "text": "System Administration\n\nwho\nShow who is logged in.\n\n\nwhoami\nDisplay current user name.\n\n\nhostname\nShow or set system’s network name.\n\n\nchsh\nChange a user’s default login shell.\n\n\ndate\nDisplay or set the date and time.\n\n\nuptime\nShow how long the system has been running.\n\n\ncal\nShows a calendar of the current month or a specified month and year.\n\n\ndf\nReport file system disk space usage.\n\n\ndu\nShow disk usage statistics.\n\n\nps\nReport a snapshot of the current processes.\n\n\nfree\nDisplays the amount of free and used memory in the system, including physical memory (RAM) and swap space.\n\n\ntop\nDisplay tasks and system status interactively.\n\n\n\ntop in iTerm2\n\n\n\n\niotop\nMonitor disk I/O usage by processes.\n\n\nhtop\nInteractive process viewer, similar to top but more configurable.\n\n\nvmstat\nReport virtual memory statistics.\n\n\ndmesg\nDisplay the kernel-related messages.\n\n\nclear\nClear the terminal screen.\n\n\nyes\nOutput a string repeatedly until killed.\nNOTE: Use Ctrl + C to interrupt the yes command.\n\n\nexit\nExits the shell or current command session.\n\n\nxargs\nBuild and execute command lines from standard input.\n\n\ncrontop\nSchedule periodic background work.\n\n\nsystemctl\nControl the systemd system and service manager.\n\n\njournalctl\nQuery and display messages from the journal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#variables",
    "href": "syntax_ref.html#variables",
    "title": "Appendix A — Syntax Reference",
    "section": "Variables",
    "text": "Variables\n\nset\nset is used to display all shell variables and functions, and to configure shell options and positional parameters.\n\n\nunset\nunset is used to remove a shell or environment variable, effectively deleting it from the current shell session.\n\n\nexport\nexportis used to set an environment variable and make it available to child processes spawned from the current shell session.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#text-processing",
    "href": "syntax_ref.html#text-processing",
    "title": "Appendix A — Syntax Reference",
    "section": "Text Processing",
    "text": "Text Processing\n\ncat\nConcatenate files and print on the standard output.\n\n\necho\nOutputs the strings it is given to the terminal.\n\n\ngrep\nPrint lines matching a pattern.\n\n\nsort\nSort lines of text files.\n\n\nuniq\nReport or omit repeated lines.\n\n\ncut\nRemove sections from each line of files.\n\n\npaste\nMerge lines of files.\n\n\njoin\nJoin lines of two files on a common field.\n\n\ncomm\nCompare two sorted files line by line.\n\n\ndiff\nCompare files line by line.\n\n\nwc\nPrint newline, word, and byte counts for each file.\n\n\nseq\nThe seq command generates a sequence of numbers, with customizable start, stop, and step values, and outputs them to standard output.\n\n\nsed\nStream editor for filtering and transforming text.\n\n\nawk\nProgramming language for data extraction and reporting.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#networking",
    "href": "syntax_ref.html#networking",
    "title": "Appendix A — Syntax Reference",
    "section": "Networking",
    "text": "Networking\n\nping\nCheck the network connection to another host.\n\n\nnetstat\nDisplay network connections, routing tables, interface statistics, masquerade connections, and multicast memberships.\n\n\nss\nUtility to investigate sockets.\n\n\nifconfig\nConfigure or display network interface parameters for a network using TCP/IP.\n\n\ntraceroute\nPrint the route packets take to network host.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#file-archiving-transfer",
    "href": "syntax_ref.html#file-archiving-transfer",
    "title": "Appendix A — Syntax Reference",
    "section": "File Archiving & Transfer",
    "text": "File Archiving & Transfer\n\ntar\nStore and extract files from a tape or disk archive.\n\n\ngzip\nCompress or expand files.\n\n\nzip\nPackage and compress (archive) files.\n\n\nscp\nSecure copy (remote file copy program).\n\n\nrsync\nFast, versatile, remote (and local) file-copying tool.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#user-management",
    "href": "syntax_ref.html#user-management",
    "title": "Appendix A — Syntax Reference",
    "section": "User Management",
    "text": "User Management\n\nchown\nThe chown command changes the owner and/or group of specified files or directories, thereby altering who has control over these resources.\n\n\nchmod\nThe chmod command changes the access permissions of file system objects (files and directories), allowing users to specify who can read, write, or execute them.\n\n\nsudo\nExecute a command as another user, typically the superuser.\n\n\nuseradd\nCreate a new user or update default new user information.\n\n\nusermod\nModify a user’s system information.\n\n\nuserdel\nDelete a user account and related files.\n\n\ngroupadd\nCreate a new group.\n\n\npasswd\nUpdate a user’s authentication tokens/password.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#disk-management",
    "href": "syntax_ref.html#disk-management",
    "title": "Appendix A — Syntax Reference",
    "section": "Disk Management",
    "text": "Disk Management\n\nfdisk\nPartition table manipulator for Linux.\n\n\nparted\nA partition manipulation program.\n\n\nmkfs\nBuild a Linux filesystem on a device, usually a hard disk partition.\n\n\nmount\nMount a filesystem.\n\n\numount\nUnmount file systems.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Appendix B — Data files",
    "section": "",
    "text": "Alan J. Perlis’ Epigrams\nThe ajperlis_epigrams.txt file contains a collection of witty programming and systems design epigrams by Alan J. Perlis.1 These statements cover various topics related to computer programming, software architecture, and philosophy. The epigrams offer nuggets of reflective and forward-thinking wisdom that provoke thought on the complexities of technology, the art of programming, and the interplay between human cognition and computational logic. This collection is invaluable for programmers and computer scientists interested in the cultural and intellectual history of computing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#music-videos",
    "href": "data.html#music-videos",
    "title": "Appendix B — Data files",
    "section": "Music Videos",
    "text": "Music Videos\nThe file music_vids.tsv is a tab-separated values (TSV) data file that details some of the most expensive music videos ever produced.2 Key information captured in the file includes:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrank\nThe music video’s position based on the production cost.\n\n\ntitle\nThe name of the music video.\n\n\nartists\nThe artist(s) who performed the song.\n\n\ndirector\nThe director of the music video.\n\n\nyear\nThe year the music video was released.\n\n\ncost_nominal\nThe production cost at the release time, presented in U.S. dollars.\n\n\ncost_adj\nThe production cost adjusted to current values for inflation, also presented in U.S. dollars.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#passwords",
    "href": "data.html#passwords",
    "title": "Appendix B — Data files",
    "section": "Passwords",
    "text": "Passwords\npwrds.csv is a CSV (comma-separated values) file containing a comprehensive list of commonly used passwords and attributes.3 The dataset contains information about passwords’ strength, popularity rank, and resilience against online attacks. The dataset includes the following variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npassword\nThe actual password text.\n\n\nrank\nNumerical ranking based on the frequency or commonness of the password.\n\n\nstrength\nA numerical value representing the estimated password strength, where higher numbers indicate stronger passwords.\n\n\nonline_crack\nAn estimate of how long it would take to crack this password using online attack methods, expressed in units from seconds to years.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#roxanne",
    "href": "data.html#roxanne",
    "title": "Appendix B — Data files",
    "section": "Roxanne",
    "text": "Roxanne\nThe file roxanne.txt contains the lyrics to the song “Roxanne” by The Police.4 The structure of the lyrics emphasizes the repeated refrain, “You don’t have to put on the red light,” which is a metaphor for not having to engage in prostitution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#trees",
    "href": "data.html#trees",
    "title": "Appendix B — Data files",
    "section": "Trees",
    "text": "Trees\nThe file trees.csv is a comma-separated value (CSV) document that catalogs some of the tallest trees in the world, providing detailed information on each tree listed.5 This file is structured to provide a quick reference to some of the most significant trees around the world, highlighting their impressive heights and the diverse locations they inhabit. Each entry in the dataset includes the following fields:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntree\nIdentifier or common name used for the tree.\n\n\nspecies\nScientific name of the tree species.\n\n\nclass\nBiological classification (e.g., Conifer, Flowering plant).\n\n\nht_meters\nHeight of the tree in meters.\n\n\nht_feet\nHeight of the tree in feet.\n\n\nlocation\nSpecific location where the tree is found.\n\n\ncontinent\nContinent on which the tree is located.\n\n\nname\nThe colloquial name given to the tree, if applicable.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#video-game-hall-of-fame",
    "href": "data.html#video-game-hall-of-fame",
    "title": "Appendix B — Data files",
    "section": "Video Game Hall of Fame",
    "text": "Video Game Hall of Fame\nThe file vg_hof.csv is a comma-separated values (CSV) document that contains a list of video games inducted into a hall of fame over several years, from 2015 to 2024.6 Each record in the dataset includes the year of induction, the game’s name, the developer, and the year the game was initially released.\nThe fields detailed in the dataset are:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nThe year the game was inducted into the hall of fame.\n\n\ngame\nThe name of the video game.\n\n\ndeveloper\nThe company or individual who developed the game.\n\n\nyear_released\nThe original year of the game’s release.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#world-health-organization-tuberculosis",
    "href": "data.html#world-health-organization-tuberculosis",
    "title": "Appendix B — Data files",
    "section": "World Health Organization Tuberculosis",
    "text": "World Health Organization Tuberculosis\nwho_tb_data.csv and who_tb_data.tsv are comma and tab-separated values (CSV and TSV) files that provides tuberculosis (TB) case data alongside population figures for selected countries over specific years, as reported by the World Health Organization (WHO).7\nThe dataset includes the following fields:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncountry\nThe name of the country where the data was recorded.\n\n\nyear\nThe year in which the data was collected.\n\n\ntype\nA descriptor of the data type, which can be either ‘cases’ indicating the number of tuberculosis cases reported, or ‘population’ representing the total population of the country in that year.\n\n\ncount\nThe numerical value corresponding to the type, either the number of TB cases or the population size.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#wu-tang-clan",
    "href": "data.html#wu-tang-clan",
    "title": "Appendix B — Data files",
    "section": "Wu-Tang Clan",
    "text": "Wu-Tang Clan\nwu_tang.csv is a comma-separated data file that provides the stage names and real names of the members of the Wu-Tang Clan, a highly influential hip-hop group.8 Each entry in the dataset correlates a member’s popularly known stage name with their legal name, offering insight into the identities behind the personas. The data structure includes two main fields:\n\n\n\nVariable\nDescription\n\n\n\n\nMember\nThe stage name of the Wu-Tang Clan member.\n\n\nName\nThe real name of the member.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "Appendix B — Data files",
    "section": "",
    "text": "Read the Wikipedia or download the original PDF.↩︎\nFrom the List of most expensive music videos on Wikipedia↩︎\nThe original data comes from Information is Beautiful↩︎\nRead more about Roxanne (The Police song) on the Wikipedia page.↩︎\nFrom the List of tallest trees on Wikipedia.↩︎\nFrom the World Video Game Hall of Fame Wikipedia page.↩︎\nFrom the WHO global tuberculosis programme↩︎\nFrom the Wu-Tang Clan Wikipedia page.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data files</span>"
    ]
  },
  {
    "objectID": "fish.html",
    "href": "fish.html",
    "title": "Appendix C — Fish",
    "section": "",
    "text": "The  Fish shell\nFish, or the Friendly Interactive SHell, is a smart and user-friendly command line shell for Linux-like operating systems. It’s designed to be more interactive and user-friendly than traditional shells like Bash or Zsh.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Fish</span>"
    ]
  },
  {
    "objectID": "fish.html#the-fish-shell",
    "href": "fish.html#the-fish-shell",
    "title": "Appendix C — Fish",
    "section": "",
    "text": "Key features\nAutosuggestions\nFish suggests commands as you type based on history and completions, just like a web browser. This feature allows users to see and reuse previous commands by simply pressing the right arrow key to complete the suggested command, which can significantly speed up typing and reduce errors.\nSyntax Highlighting\nOne of Fish’s most noticeable features is its real-time syntax highlighting. Commands that are valid change color as you type them. It also helps users catch errors before the command is executed, such as highlighting misspelled commands or incorrect paths in red.\nWeb-Based Configuration\nFish includes a web-based configuration interface (accessible via the fish_config command), which makes customizing the shell settings and prompt easier for users who prefer a graphical interface over editing configuration files manually (or if you’re new to the command line).\nEnhanced Tab Completion\nFish provides intelligent tab completions for commands, file names, variables, and user-defined functions. It not only completes based on the prefix but also considers the whole line context, making the completions more relevant.\nImproved Variables and Scoping\nFish simplifies variable management, including universal variables that are automatically shared between all running shells and persist across restarts without needing explicit saving to a file. Variable scoping is also more straightforward, helping avoid common bugs seen in other shells.\nFunction Autoloads\nFish allows functions to be defined in individual files and automatically loads them only when needed. This lazy-loading of functions helps speed up the start time of the shell.\nExtensible\nFish is designed to be easily extensible through plugins. The Fisherman and Oh My Fish frameworks offer many plugins and themes designed to enhance Fish’s capabilities or customize its appearance.\nMan Page Completions\nFish generates command completions automatically from man pages, which means it often supports completions for all the installed commands without needing special configuration.\nNo Configuration Needed\nFish is designed to work properly out of the box, without needing to configure it extensively. This makes it very accessible for new users or those who want a powerful shell without the need to customize or configure it heavily.\nUser-Friendly Scripts\nFish uses a syntax that is slightly different from the traditional POSIX shell syntax, which is often simpler and easier to understand. For example, loops and conditionals are clearer, and there is no need for explicit subshell management.\n\n\nCustomizations\n Fish\nFish customizations are generally done through the config.fish file, typically located in ~/.config/fish/. Fish makes it easy to customize the prompt using the fish_prompt function.\nfunction fish_prompt\n    set_color green\n    echo -n (whoami) \"@\" (hostname) \" \"\n    set_color blue\n    echo -n (prompt_pwd)\n    set_color normal\n    echo -n \"&gt; \"\nend\nThis prompt shows the username in green, the hostname, and the current working directory in blue, followed by a &gt; symbol.\nAliases: Fish uses the alias command similar to Bash and Zsh.\nalias ll=\"ls -la\"\nFunctions: In Fish, functions are first-class citizens and are easy to define.\nfunction mkcd\n    mkdir -p $argv; and cd $argv\nend\n\n\nAdvanced\nUniversal Variables\nFish supports universal variables that persist across sessions and are shared among all Fish instances.\nset -U fish_greeting \"Welcome to Fish Shell\"\nPlugins and Themes\nFish has a growing ecosystem of plugins and themes, often managed by the fisher plugin manager.\ncurl -sL https://git.io/fisher | source && fisher install jorgebucaran/fisher\nfisher install oh-my-fish/theme-bobthefish",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Fish</span>"
    ]
  },
  {
    "objectID": "fish.html#footnotes",
    "href": "fish.html#footnotes",
    "title": "Appendix C — Fish",
    "section": "",
    "text": "Ars Technica has a great summary comparing Fish to other shells.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Fish</span>"
    ]
  },
  {
    "objectID": "macos.html",
    "href": "macos.html",
    "title": "Appendix D — macOS",
    "section": "",
    "text": "macOS Terminal\nUsing Unix/Linux on macOS offers a unique and powerful environment with macOS’s Terminal, but it’s important to be aware of the differences and limitations inherent in macOS’s implementation of Unix. Below we cover some special considerations rooted in macOS’s distinct architecture and its version of Unix.\nThe default Terminal app on macOS provides a solid interface to access the Unix command line, but there are other terminal emulators available (like iTerm2) that offer additional features such as split panes, search, and customization options.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "macos.html#recap",
    "href": "macos.html#recap",
    "title": "Appendix D — macOS",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Appendix E — Quarto",
    "section": "",
    "text": "Install\nIt is particularly useful for those looking to combine Unix/Linux command line operations with document creation, offering a way to embed executable code within documents. The next sections will guide you through downloading and installing Quarto, .qmd documents, YAML headers and Bash code chunks.\nQuarto can be installed from its official website. Follow the platform-specific instructions to install it on your system. Make sure Quarto has been installed correctly and is available in your system’s PATH.\nCheck if Quarto is in PATH:\nquarto --version\n## 1.5.28\nIf the Quarto version isn’t on your PATH, you’ll need to add the location of your quarto installation to PATH. You can do that with the commands below (depending on your shell).\nYou can also use which to locate quarto path:\nwhich quarto\n## /usr/local/bin/quarto\nOn macOS, you can use find in the Terminal:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#install",
    "href": "quarto.html#install",
    "title": "Appendix E — Quarto",
    "section": "",
    "text": "Bash:1\n\n\necho 'export PATH=\"$PATH:/path/to/quarto\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n\n\n\nZsh:2\n\n\necho 'export PATH=\"$PATH:/path/to/quarto\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n\n\n\n\n\nfind / -name quarto 2&gt;/dev/null",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#development",
    "href": "quarto.html#development",
    "title": "Appendix E — Quarto",
    "section": "Development",
    "text": "Development\nIf you decide to use Quarto documents, you’ll need to decide on the tool (or development environment) you want to use. At the time of this writing, Quarto has tutorials for getting started in RStudio, VS Code, Jupyter, Neovim, or a text editor (like Sublime Text).3 I recommend using an IDE like Visual Studio Code (VS Code) for several reasons:4\n\nIDE Features\nVS Code offers typical IDE features (syntax highlighting, code completion, linting, formatting, debugging, etc.) and a Quarto extension. VS Code also has a wide range of extensions for Bash scripting that can significantly streamline development.5\n\n\n\n\n\n\n\nExtensibility\nVS Code runs on multiple platforms (Windows, macOS, and Linux), which means you can maintain scripts across different environments using the same tool.\n\n\n\n\n\n\nVS Code also supports remote development (i.e., the ability to write and debug scripts running on remote servers and containers from your local machine).\n\n\n\n\n\n\n\n\n\n\n\n\nGit Integration\nVS Code also has excellent support for version control systems, especially Git.\n\n\n\n\n\n\nThis integration makes it easier to track changes, revert to previous versions of scripts, and manage updates, all within the same editor environment.\n\n\n\n\n\n\n\n\n\n\n\n\nCustomization\nVS Code can be customized and configured to suit each user’s specific needs, including setting up a personalized development environment with their preferred settings, key bindings, and even look and feel.\n\n\n\n\n\n\n\nQuarto Preview\nQuarto documents can be configured to render automatically, which gives users the ability to view document outputs as they’re developed in real-time.\n\n\n\n\n\n\n\nShell integration\nA Terminal window is available in the IDE as a separate window, giving quick access to the command line (with multiple shell options):\n\n\n\n\n\nOverall, VS Code provides a powerful, versatile, and user-friendly platform to develop, maintain, and manage Shell scripts. There is also a large community around VS Code with lots of tutorials, guides, and forums where users can find support.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-documents",
    "href": "quarto.html#quarto-documents",
    "title": "Appendix E — Quarto",
    "section": "Quarto Documents",
    "text": "Quarto Documents\nQuarto documents are a flexible and powerful tool for creating dynamic and reproducible reports, presentations, and publications. Quarto documents can support multiple programming languages, including R, Python, Julia, and Bash. Users can combine narrative text with code in a single document, rendering it into various formats like HTML, PDF, and Word.\nThis feature proves not only invaluable for data scientists, but for anyone looking to learn a new programming language. Quarto documents are a perfect ‘sandbox’ to experiment with code, take notes, and review the outputs. Moreover, Quarto has the supports cross-referencing, advanced layout options, and beautiful web and book publishing workflows.6\nQuarto documents consist of the following elements:\n\nYAML Header: This is used to specify the document’s title, author, and output format.\nNarrative Text: Plain language explanations and context.\nCode Chunks: Executable code blocks from languages like R, Python, Bash, or Julia.\n\nAdditional features include:\n\nResults: the output is displayed directly from the executable code chunks (tables, graphs, etc.).\nCross-references and Citations: Link to figures, tables, sections, or reference bibliographic sources.\nFigures and Tables: Markdown syntax can be used to add images and tables.\nAppendices and Bibliography: Footnotes and references.\n\n\n\n\n\n\n\nLiterate Programming\n\n\n\n\n\n\nLiterate programming is a programming paradigm that intertwines code with human-readable narrative, allowing programmers to write programs in the order best suited for human understanding. Donald Knuth developed this approach to make code more understandable and maintainable.\n\n‘Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.’ - Donald Knuth. Literate Programming (1984) in Literate Programming. CSLI, 1992, pg. 99.\n\nLiterate programming can foster clearer communication of complex programming concepts and enhances collaboration among developers.\n\n\n\n\n\nYAML header\nYAML is a lightweight markup language that’s easy to write and read. In Quarto, the YAML header is used to configure document properties such as the title, engine, output format, and more. It serves as the foundation for controlling how your Quarto document behaves and appears.\nQuarto documents are written in markdown and can include executable code in various programming languages, including Unix commands. The YAML header is placed between three dashes --- at the top of each Quarto document to specify metadata and global options.\n---\ntitle: \"Using Bash\"\n---\nTo run Bash commands, specify knitr in the engine field of in the YAML header of the Quarto file, and any additional key-value pairs:7\n---\ntitle: \"Using Bash\"\nengine: knitr\nknitr:\n  opts_chunk: \n    collapse: true\n---\n\n\nCode Chunks\nOne of the powerful features of Quarto is the ability to integrate executable code chunks into Markdown documents. You can create bash code chunks using the following syntax:\n```{bash}\necho \"foo\" \n```\nBash code chunks allow you to include executable commands within your Quarto documents. You can also specify the code chunk options with the hash-pipe (#|):8\n```{bash}\n#| code-fold: show\n#| code-summary: 'show/hide echo'\necho \"foo\" \n```\nWhen the document is rendered, the narrative text is included with the output from the commnads.\n\n\nshow/hide echo\necho \"foo\"\n## foo\n\n\nThis simplicity allows authors to focus on their content rather than formatting.\n\n\nCode Chunk Isolation\nWhen incorporating Bash code chunks into Quarto documents, an essential detail to remember is the behavior of the working directory during file rendering. By default, Quarto sets the working directory to the location of the current document within the project.\nConsider the following scenario in a Quarto project:\n\n# This code chunk displays the current working directory\npwd\n## /Users/mjfrigaard/projects/books/fm-unix\n\nAlthough we can navigate to a different directory within a given code chunk:\n\ncd data # Change the current working directory to 'data' \npwd # confirm the change\n## /Users/mjfrigaard/projects/books/fm-unix/data\n\nIt’s crucial to note that Quarto resets the working directory to the document’s location for each new code chunk:\n\n# Verifying the working directory, which reverts to \npwd # the document's location for each new code chunk\n## /Users/mjfrigaard/projects/books/fm-unix\n\nThis behavior is different than what we’d see in Posit Workbench’s Terminal on my local machine:\n\nThe current working directory is ~/projects/books/fm-unix/ (as indicated by the Terminal prompt):\n\n\n\n\nfm-unix/ directory\n\n\n\nIf we change the working directory with .., we are in the books/ directory (as indicated by the books in the Shell window):\n\n\n\n\nbooks/ directory\n\n\n\nWhen we check the current working directory again with pwd, we see the location is still ~/projects/books/.\n\n\n\n\nStill in the books/ directory\n\n\nThis difference in behavior can be frustrating, but it also means we’ll start with a ‘clean slate’ in each new code chunk!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#recap",
    "href": "quarto.html#recap",
    "title": "Appendix E — Quarto",
    "section": "Recap",
    "text": "Recap\nThis chapter covered a guide on how to use Quarto for embedding and executing Bash code, a powerful feature for Unix/Linux users and developers who need to document and automate command line tasks.\n\nSetting Up Quarto: We covered the resources and prerequisites for installing Quarto, and how to use Quarto within Visual Studio Code (VS Code). We included setting VS Code with the Quarto extension, and options for configuring the IDE.\nWriting and Executing Bash Code: We discussed setting up Bash in Quarto documents, including how to configure the YAML header in the Quarto document to include Bash as the programming language, and ensuring Quarto documents can recognize and execute Bash code.\n\nVS Code and Quarto are powerful tools. The IDE provides a user-friendly interface, extensive plugin ecosystem, and strong support for Quarto’s interactive capabilities.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "Appendix E — Quarto",
    "section": "",
    "text": "Bash is common in Linux and older macOS versions↩︎\nZsh is now the default in shell in macOS↩︎\nFor what its worth, I’ve been using RStudio for years and love it. However, I’ve also used Quarto in VS Code and love many of it’s features.↩︎\nIf you prefer a ‘Free/Libre Open Source Software Binary of VS Code’, check out vscodium↩︎\nBash Debug and Bash IDE are VS Code extensions specifically designed for Bash scripting, which we’ll cover in a later chapter.↩︎\nIn fact, this entire book was written using Quarto documents.↩︎\nRead more about configuring shell code blocks in Quarto in the documentation.↩︎\nConsult the full list of code chunk options in the Quarto documentation.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Bash\nBash, short for Bourne Again SHell, is a command line interface and scripting language for operating systems, enabling direct command input and task automation. Originally created for the GNU project and known for its flexibility and powerful features, Bash is the standard shell on many Linux distributions and was the default shell in the Terminal on macOS until the Catalina release.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-nfs",
    "href": "glossary.html#sec-nfs",
    "title": "Glossary",
    "section": "Network File System (NFS)",
    "text": "Network File System (NFS)\nNFS is a file system protocol that enables a user to access files over a network. It provides a central location for storing and sharing files across multiple computers and allows users to work with files on remote servers as if they were on their local machine.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-onprem",
    "href": "glossary.html#sec-onprem",
    "title": "Glossary",
    "section": "On-prem",
    "text": "On-prem\nOn-prem or on-premises refers to software and technology installed and running on computers on the premises of the user instead of a remote facility. On-prem can include data centers, servers, and other hardware within a company’s property. It is chosen for greater control over the computing environment and compliance reasons, as the organization is responsible for managing the security, maintenance, and updating of the systems.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-quarto",
    "href": "glossary.html#sec-quarto",
    "title": "Glossary",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing framework designed to work with R, Python, Julia, Observable JavaScript, and more, making it a versatile tool for data scientists, researchers, and anyone involved in data analysis.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-yaml",
    "href": "glossary.html#sec-yaml",
    "title": "Glossary",
    "section": "YAML",
    "text": "YAML\nYAML: YAML is a human-friendly data format for configuration files and data exchange, using key-value pairs, lists, and indentation to organize data.\nkey: value\n  key: value\nIt’s readable and easily parsed by machines, making it popular for application configuration and data sharing.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  }
]